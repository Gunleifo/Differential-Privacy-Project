{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Privacy and Accuracy in Machine Learning Models with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores how **Differential Privacy Stochastic Gradient Descent (DP-SGD)** and **Model Agnostic Private Learning (MAPL)** impact machine learning models. Specifically, we examine whether DP techniques interfere with model accuracy and the evolution of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup - Install Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if there are missing packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pandas numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install ucimlrepo\n",
    "# %pip install imblearn\n",
    "# %pip install tabulate\n",
    "# %pip install boruta\n",
    "# %pip install torch\n",
    "# %pip install opacus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing the relevant libraries and getting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo #to retrieve the dataset\n",
    "\n",
    "# Getting the data\n",
    "dataset_id = 891  # our chosen dataset\n",
    "dataset = fetch_ucirepo(id=dataset_id)\n",
    "df = dataset.data.original\n",
    "\n",
    "# # Previous way to retrieve the dataset (for documentation)\n",
    "# url = 'https://archive.ics.uci.edu/static/public/891/data.csv'\n",
    "# df = pd.read_csv(url)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the dataset contains 253,680 records with 23 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking basic Dataset Information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that all features are stored as integer data types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the features and their descriptions:\n",
    "\n",
    "| Variable Name     | Role    | Type    | Description                                                                 |\n",
    "|-------------------|---------|---------|-----------------------------------------------------------------------------|\n",
    "| ID                | ID      | Integer | Patient ID                                                                  |\n",
    "| Diabetes_binary   | Target  | Binary  | 0 = no diabetes 1 = prediabetes or diabetes                                 |\n",
    "| HighBP            | Feature | Binary  | 0 = no high BP 1 = high BP                                                  |\n",
    "| HighChol          | Feature | Binary  | 0 = no high cholesterol 1 = high cholesterol                                |\n",
    "| CholCheck         | Feature | Binary  | 0 = no cholesterol check in 5 years 1 = yes cholesterol check in 5 years    |\n",
    "| BMI               | Feature | Integer | Body Mass Index                                                             |\n",
    "| Smoker            | Feature | Binary  | Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] 0 = no 1 = yes |\n",
    "| Stroke            | Feature | Binary  | (Ever told) you had a stroke. 0 = no 1 = yes                                |\n",
    "| HeartDiseaseorAttack | Feature | Binary | coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes  |\n",
    "| PhysActivity      | Feature | Binary  | physical activity in past 30 days - not including job 0 = no 1 = yes        |\n",
    "| Fruits            | Feature | Binary  | Consume Fruit 1 or more times per day 0 = no 1 = yes                        |\n",
    "| Veggies           | Feature | Binary  | Consume Vegetables 1 or more times per day 0 = no 1 = yes                   |\n",
    "| HvyAlcoholConsump | Feature | Binary  | Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) 0 = no 1 = yes |\n",
    "| AnyHealthcare     | Feature | Binary  | Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no 1 = yes |\n",
    "| NoDocbcCost       | Feature | Binary  | Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes |\n",
    "| GenHlth           | Feature | Integer | General health: 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor |\n",
    "| MentHlth          | Feature | Integer | For how many days during the past 30 days was your mental health not good? scale 1-30 days |\n",
    "| PhysHlth          | Feature | Integer | For how many days during the past 30 days was your physical health not good? scale 1-30 days |\n",
    "| DiffWalk          | Feature | Binary  | Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes   |\n",
    "| Sex               | Feature | Binary  | 0 = female 1 = male                                                    |\n",
    "| Age               | Feature | Integer | 13 levels, 1 = 18-24, 9 = 60-64, 13 = 80 or older              |\n",
    "| Education         | Feature | Integer | 6 levels, 1 = Never attended school or only kindergarten, 2 = Grades 1 through 8, 3 = Grades 9 through 11, 4 = Grade 12 or GED, 5 = College 1 year to 3 years, 6 = College 4 years or more |\n",
    "| Income            | Feature | Integer | 8 levels, 1 = less than $10,000, 5 = less than $35,000, 8 = $75,000 or more |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed basic statistics for the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that there are no missing values (as stated in the dataset description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the dataset only has 13.93% records with diabetes. We will address this imbalance later during data cleaning and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of dataset\n",
    "diabetes_counts = df['Diabetes_binary'].value_counts()\n",
    "print(\"Distribution of target variable:\")\n",
    "print(diabetes_counts)\n",
    "print(f\"Percentage of records with diabetes: {diabetes_counts[1]/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed a basic relationship between the features using a correlation matrix. We still need to explain what we can see from this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in preprocessing we would handle missing values, encoding binary data (True -> 1), encoding categorical data, feature scaling. However, since the dataset is already clean and preprocessed, we will only address the class imbalance issue.\n",
    "In the balanced datased, we have reduced the number of records with diabetes to match the number of records without diabetes, so that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_with_diabetes = df['Diabetes_binary'].value_counts()[1]\n",
    "\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "print(f\"- Total samples in the original dataset: {len(df)}\")\n",
    "print(f\"- Samples with diabetes (class 1): {num_with_diabetes}\")\n",
    "print(f\"- Samples without diabetes (class 0): {df['Diabetes_binary'].value_counts()[0]}\")\n",
    "\n",
    "df_no_diabetes = df[df['Diabetes_binary'] == 0].sample(n=num_with_diabetes, random_state=42)\n",
    "df_with_diabetes = df[df['Diabetes_binary'] == 1]\n",
    "\n",
    "df_balanced = pd.concat([df_no_diabetes, df_with_diabetes])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_balanced_noTarget = df_balanced.drop(columns=[\"Diabetes_binary\", \"ID\"])\n",
    "y_balanced = df_balanced[\"Diabetes_binary\"]\n",
    "\n",
    "print(\"\\nBalanced Dataset:\")\n",
    "print(f\"- Total samples in the balanced dataset: {len(df_balanced)}\")\n",
    "print(df_balanced['Diabetes_binary'].value_counts())\n",
    "\n",
    "print(f\"- Samples without diabetes (class 0): {df_balanced['Diabetes_binary'].value_counts()[0]}\")\n",
    "print(f\"- Samples with diabetes (class 1): {df_balanced['Diabetes_binary'].value_counts()[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explain which features we have chosen based on the data exploration above and the following feature selection methods. We can use variance threshold, correlation matrix, kbest, rfe, boruta..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import (\n",
    "    RFE,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "def save_results(results, file_path, columns):\n",
    "    results_df = pd.DataFrame(results, columns=columns)\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "\n",
    "def getVariances(threshold, data):\n",
    "    var_threshold = VarianceThreshold(threshold=threshold)\n",
    "    var_threshold.fit(data)\n",
    "    variances = var_threshold.variances_\n",
    "    # Create a DataFrame for variances\n",
    "    variances_df = pd.DataFrame(variances, index=data.columns, columns=[\"Variance\"])\n",
    "    # Sort variances in descending order\n",
    "    variances_df = variances_df.sort_values(by=\"Variance\", ascending=False)\n",
    "    # Features with variance >= threshold\n",
    "    features_high_variance = variances_df[variances_df[\"Variance\"] >= threshold]\n",
    "    # Features with variance < threshold\n",
    "    features_low_variance = variances_df[variances_df[\"Variance\"] < threshold]\n",
    "    return variances_df, features_high_variance, features_low_variance\n",
    "\n",
    "def getKBest(k, data, target):\n",
    "    # selection\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    x_kbest = selector.fit_transform(data, target)\n",
    "    # Create a DataFrame with the selected features\n",
    "    kbest_features = data.columns[selector.get_support()]\n",
    "    x_kbest_df = pd.DataFrame(x_kbest, columns=kbest_features)\n",
    "\n",
    "    return x_kbest_df\n",
    "\n",
    "def process_kbest(data, target, k, file_path):\n",
    "    try:\n",
    "        x_kbest_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        x_kbest_df = getKBest(k, data, target)\n",
    "        x_kbest_df[target.name] = target.values\n",
    "        x_kbest_df.to_csv(file_path, index=False)\n",
    "    x_kbest_df = x_kbest_df.drop(columns=[target.name], axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_kbest_df, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    y_pred = rf_model.predict(x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    kbest_features_names = x_kbest_df.columns.tolist()\n",
    "    \n",
    "    return (k, accuracy, precision, recall, f1, kbest_features_names)\n",
    "\n",
    "def getRFE(k, data, target):\n",
    "    # selection\n",
    "    selector = RFE(estimator=rf_model, n_features_to_select=k)\n",
    "    x_rfe = selector.fit_transform(data, target)\n",
    "\n",
    "    # Create a DataFrame with the selected features\n",
    "    rfe_features = data.columns[selector.get_support()]\n",
    "    x_rfe_df = pd.DataFrame(x_rfe, columns=rfe_features)\n",
    "\n",
    "    return x_rfe_df\n",
    "\n",
    "def process_rfe(data, target, k, file_path):\n",
    "    try:\n",
    "        x_rfe_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        x_rfe_df = getRFE(k, data, target)\n",
    "        x_rfe_df[target.name] = target.values\n",
    "        x_rfe_df.to_csv(file_path, index=False)\n",
    "    x_rfe_df = x_rfe_df.drop(columns=[target.name], axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_rfe_df, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    y_pred = rf_model.predict(x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    rfe_features_names = x_rfe_df.columns.tolist()\n",
    "    return (k, accuracy, precision, recall, f1, rfe_features_names)\n",
    "\n",
    "def apply_boruta(data, target):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    boruta = BorutaPy(\n",
    "        rf_model,\n",
    "        n_estimators=\"auto\",\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    boruta.fit(x_train.values, y_train.values)\n",
    "    sel_x_train = boruta.transform(x_train.values)\n",
    "    sel_x_test = boruta.transform(x_test.values)\n",
    "    rf_model.fit(sel_x_train, y_train)\n",
    "    y_pred = rf_model.predict(sel_x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    selected_features_mask = boruta.support_\n",
    "    selected_features = x_train.columns[selected_features_mask].tolist()\n",
    "    return (accuracy, precision, recall, f1, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances_df, features_high_variance, features_low_variance = getVariances(\n",
    "    0, df_balanced_noTarget\n",
    ")\n",
    "\n",
    "# Plot variances of all features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(variances_df.index, variances_df['Variance'], color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Feature Variances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Highlight features with high and low variance\n",
    "print(\"\\nFeatures with High Variance:\")\n",
    "print(features_high_variance)\n",
    "\n",
    "print(\"\\nFeatures with Low Variance:\")\n",
    "print(features_low_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_balanced.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on correlations with the target variable\n",
    "diabetes_correlations = correlation_matrix['Diabetes_binary'].sort_values(ascending=False)\n",
    "print(\"\\nFeatures correlated with Diabetes (sorted):\")\n",
    "print(diabetes_correlations)\n",
    "\n",
    "# Visualize correlations with the target\n",
    "plt.figure(figsize=(12, 8))\n",
    "diabetes_correlations[1:].plot(kind='bar')  # Exclude self-correlation\n",
    "plt.title(\"Feature Correlation with Diabetes\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "columns = [\"Model\", \"K\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "for k in range(1, df_balanced_noTarget.shape[1] + 1):\n",
    "    file_path = f\"featureSelectionData/kbest/k/{k}best_features.csv\"\n",
    "    kbestResult = process_kbest(df_balanced_noTarget, y_balanced, k, file_path)\n",
    "    results.append((\"rf\",) + kbestResult)\n",
    "save_results(\n",
    "    results, f\"featureSelectionData/kbest/kbest_rf_results.csv\", columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "columns = [\"Model\", \"K\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "for k in range(1, df_balanced_noTarget.shape[1] + 1):\n",
    "    file_path = f\"featureSelectionData/rfe/k/{k}rfe_features.csv\"\n",
    "    rfeResult = process_rfe(df_balanced_noTarget, y_balanced, k, file_path)\n",
    "    results.append((\"rf\",) + rfeResult)\n",
    "save_results(results, f\"featureSelectionData/rfe/rfe_rf_results.csv\", columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "accuracy, precision, recall, f1, selected_features = apply_boruta(\n",
    "    df_balanced_noTarget, y_balanced\n",
    ")\n",
    "save_results(\n",
    "    [(\"rf\", accuracy, precision, recall, f1, selected_features)],\n",
    "    f\"featureSelectionData/boruta/boruta_rf_results.csv\",\n",
    "    columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Model without Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing the relevant libraries to train our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Data Splitting Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our models we have to split the data into a training set and a test set. We have chosen a 80/20 split, which ensures we have enough training data for the model to learn patterns and enough data for performance evaluation. We have used stratified sampling to ensure that both the training set and test set includes a 50/50 split of records having diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced = df_balanced.drop('Diabetes_binary', axis=1) # drop the target column\n",
    "y_balanced = df_balanced['Diabetes_binary']\n",
    "\n",
    "X_imbalanced = df.drop('Diabetes_binary', axis=1) #drop the target column\n",
    "y_imbalanced = df['Diabetes_binary']\n",
    "\n",
    "# split into train (70%) and temp (30%)\n",
    "X_train_bal, X_temp_bal, y_train_bal, y_temp_bal = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# split temp set into validation (10%) and test (20%)\n",
    "X_val_bal, X_test_bal, y_val_bal, y_test_bal = train_test_split(\n",
    "    X_temp_bal, y_temp_bal, test_size=2/3, random_state=42, stratify=y_temp_bal\n",
    ")\n",
    "\n",
    "# Imbalanced dataset\n",
    "X_train_imb, X_temp_imb, y_train_imb, y_temp_imb = train_test_split(\n",
    "    X_imbalanced, y_imbalanced, test_size=0.3, random_state=42, stratify=y_imbalanced\n",
    ")\n",
    "\n",
    "X_val_imb, X_test_imb, y_val_imb, y_test_imb = train_test_split(\n",
    "    X_temp_imb, y_temp_imb, test_size=2/3, random_state=42, stratify=y_temp_imb\n",
    ")\n",
    "\n",
    "print(f\"Balanced Training set: {X_train_bal.shape[0]} samples\")\n",
    "print(f\"Balanced Validation set: {X_val_bal.shape[0]} samples\")\n",
    "print(f\"Balanced Testing set: {X_test_bal.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in balanced training set:\")\n",
    "print(y_train_bal.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in balanced validation set:\")\n",
    "print(y_val_bal.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in balanced testing set:\")\n",
    "print(y_test_bal.value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nImbalanced Training set: {X_train_imb.shape[0]} samples\")\n",
    "print(f\"Imbalanced Validation set: {X_val_imb.shape[0]} samples\")\n",
    "print(f\"Imbalanced Testing set: {X_test_imb.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced training set:\")\n",
    "print(y_train_imb.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced validation set:\")\n",
    "print(y_val_imb.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced testing set:\")\n",
    "print(y_test_imb.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Using all features (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_bal)\n",
    "\n",
    "# Checking accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test_bal, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_bal, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Using selected features (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important features\n",
    "important_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                      'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                      'CholCheck', 'Smoker', 'NoDocbcCost', 'Sex', 'AnyHealthcare', \n",
    "                      'Income', 'Education']\n",
    "\n",
    "# Filter X_train_bal and X_test_bal to include only important features\n",
    "X_train_important = X_train_bal[important_features]\n",
    "X_test_important = X_test_bal[important_features]\n",
    "\n",
    "# Train with the balanced filtered dataset\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_important, y_train_bal)\n",
    "\n",
    "# Make predictions using the filtered test set\n",
    "y_pred = rf_model.predict(X_test_important)\n",
    "\n",
    "# Checking accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test_bal, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_bal, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision: How many predicted positives are actually positive?\n",
    "- Recall: How many actual positives were correctly identified?\n",
    "- F1-score: A balance between precision and recall.\n",
    "- Support: Number of actual occurrences of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Using selected features (imbalanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important features\n",
    "important_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                      'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                      'CholCheck', 'Smoker', 'NoDocbcCost', 'Sex', 'AnyHealthcare', \n",
    "                      'Income', 'Education']\n",
    "\n",
    "# Filter the imbalanced training and test sets to include only important features\n",
    "X_train_imb_important = X_train_imb[important_features]\n",
    "X_test_imb_important = X_test_imb[important_features]\n",
    "\n",
    "# Train with the imbalanced filtered dataset\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_imb_important, y_train_imb)\n",
    "\n",
    "# Make predictions using the filtered test set\n",
    "y_pred = rf_model.predict(X_test_imb_important)\n",
    "\n",
    "# Checking accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test_imb, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_imb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model over prioritizes the majority class (0 - no diabetes), as it was trained with the imbalanced data.**\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Balanced data (6.2) - Lower accuracy (74%) but much better at detecting class 1 - diabetes (79%), as it gives the same importance to both classes.\n",
    "\n",
    "Imbalanced data (6.3) - Higher accuracy (86%) but poor detection of class 1 - diabetes (16%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                      'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                      'CholCheck', 'Smoker', 'NoDocbcCost', 'Sex', 'AnyHealthcare', \n",
    "                      'Income', 'Education']\n",
    "\n",
    "# Filter balanced training and test sets to include only important features\n",
    "X_train_bal_important = X_train_bal[important_features]\n",
    "X_test_bal_important = X_test_bal[important_features]\n",
    "X_val_bal_important = X_val_bal[important_features]\n",
    "\n",
    "# Train with the filtered balanced dataset\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_bal_important, y_train_bal)\n",
    "\n",
    "# Make predictions using the filtered test set\n",
    "y_pred_bal = rf_model.predict(X_test_bal_important)\n",
    "\n",
    "# Checking accuracy and classification report for balanced test set\n",
    "print(\"Balanced Accuracy:\", accuracy_score(y_test_bal, y_pred_bal))\n",
    "print(\"\\nBalanced Classification Report:\")\n",
    "print(classification_report(y_test_bal, y_pred_bal))\n",
    "\n",
    "# Evaluate validation set - for documentation\n",
    "y_pred_val_bal = rf_model.predict(X_val_bal_important)\n",
    "print(\"\\nBalanced Validation Accuracy:\", accuracy_score(y_val_bal, y_pred_val_bal))\n",
    "print(\"\\nBalanced Validation Classification Report:\")\n",
    "print(classification_report(y_val_bal, y_pred_val_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Using SMOTE balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "important_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                      'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                      'CholCheck', 'Smoker', 'Veggies', 'HvyAlcoholConsump', 'PhysActivity', \n",
    "                      'Education' , 'Income']\n",
    "\n",
    "X = df[important_features]\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "# SMOTE balances the dataset by generating synthetic samples for the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y))  # Shows original class distribution\n",
    "print(\"After SMOTE:\", Counter(y_resampled))  # Shows balanced class distribution\n",
    "\n",
    "print(\"Original dataset shape:\", X.shape, y.shape)\n",
    "print(\"Resampled dataset shape:\", X_resampled.shape, y_resampled.shape)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Train with the balanced dataset split\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Checking accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.2.1 Membership Inference Attack(MIA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---1. Standard MIA---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this attack is to determine whether a given sample was used in training. \n",
    "\n",
    "It queries the model (6.6) with different data points and analyzes the confidence scores. This happens because the model behaves differently for data it has seen before and new data. As this one was trained without DP, it is easier to leak private data as it may have \"memorized\" training data. \n",
    "\n",
    "** To try with DP model(s) as well **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter #?\n",
    "\n",
    "assert 'rf_model' in locals(), \"Random Forest model (rf_model) is not defined!\" #if not defined, run 6.6\n",
    "\n",
    "# Get confidence scores (highest probability prediction) for train and test samples\n",
    "train_confidences = rf_model.predict_proba(X_train).max(axis=1)\n",
    "test_confidences = rf_model.predict_proba(X_test).max(axis=1)\n",
    "\n",
    "# Set attack threshold (mean confidence of training data)\n",
    "threshold = np.mean(train_confidences)\n",
    "\n",
    "# Attack: Predict \"1\" (member) if confidence > threshold, else \"0\" (non-member)\n",
    "train_preds = [1 if c > threshold else 0 for c in train_confidences]\n",
    "test_preds = [0 if c <= threshold else 1 for c in test_confidences]\n",
    "\n",
    "# True labels: training samples = 1 (member), test samples = 0 (non-member)\n",
    "y_true = [1] * len(train_preds) + [0] * len(test_preds)\n",
    "y_pred = train_preds + test_preds\n",
    "\n",
    "attack_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "train_mean_conf = np.mean(train_confidences)\n",
    "test_mean_conf = np.mean(test_confidences)\n",
    "\n",
    "print(f\" Accuracy: {attack_accuracy:.2f}\")\n",
    "print(f\" Avg Train Confidence: {train_mean_conf:.2f}\")\n",
    "print(f\" Avg Test Confidence: {test_mean_conf:.2f}\")\n",
    "print(f\" Attack Threshold Used: {threshold:.2f}\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_confidences, bins=50, alpha=0.6, label=\"Train (Member)\", color='blue')\n",
    "plt.hist(test_confidences, bins=50, alpha=0.6, label=\"Test (Non-Member)\", color='red')\n",
    "plt.axvline(threshold, color='black', linestyle='dashed', label=\"Attack Threshold\")\n",
    "plt.xlabel(\"Model Confidence Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Membership Inference Attack - Confidence Score Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 62% of accuracy means that the model partially leaks private information. It is possible to distinguish between training and testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 2. Real World Case MIA ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# # To keep: For development\n",
    "# real_members = X_train.head(5)\n",
    "# print(\"First 5 Real Members (Training Set):\\n\", real_members)\n",
    "\n",
    "risky_patients = [\n",
    "    # If adding more patients, make sure that the attributes have valid values according to the dataset\n",
    "\n",
    "    {\n",
    "        'GenHlth': 3, 'HighBP': 1, 'DiffWalk': 0, 'BMI': 21, 'HighChol': 0, 'Age': 12, \n",
    "        'HeartDiseaseorAttack': 0, 'PhysHlth': 0, 'Stroke': 0, 'MentHlth': 0, 'CholCheck': 1, \n",
    "        'Smoker': 0, 'Veggies': 1, 'HvyAlcoholConsump': 0, 'PhysActivity': 1, 'Education': 4, \n",
    "        'Income': 4\n",
    "    },  # Patient used in the test_set. ID nr 178592\n",
    "\n",
    "    {\n",
    "        'GenHlth': 2, 'HighBP': 1, 'DiffWalk': 0, 'BMI': 39, 'HighChol': 1, 'Age': 7, \n",
    "        'HeartDiseaseorAttack': 0, 'PhysHlth': 0, 'Stroke': 0, 'MentHlth': 0, 'CholCheck': 1, \n",
    "        'Smoker': 0, 'Veggies': 1, 'HvyAlcoholConsump': 0, 'PhysActivity': 1, 'Education': 4, \n",
    "        'Income': 1\n",
    "    },  # Patient used in the test_set. ID nr 318886\n",
    "\n",
    "    {\n",
    "        'GenHlth': 4, 'HighBP': 0, 'DiffWalk': 1, 'BMI': 23, 'HighChol': 0, 'Age': 8, \n",
    "        'HeartDiseaseorAttack': 1, 'PhysHlth': 24, 'Stroke': 0, 'MentHlth': 22, 'CholCheck': 0, \n",
    "        'Smoker': 1, 'Veggies': 0, 'HvyAlcoholConsump': 1, 'PhysActivity': 0, 'Education': 2, \n",
    "        'Income': 1\n",
    "    },  # Random person created for testing purposes\n",
    "\n",
    "    {\n",
    "        'GenHlth': 5, 'HighBP': 1, 'DiffWalk': 1, 'BMI': 40, 'HighChol': 1, 'Age': 9, \n",
    "        'HeartDiseaseorAttack': 0, 'PhysHlth': 15, 'Stroke': 0, 'MentHlth': 18, 'CholCheck': 1, \n",
    "        'Smoker': 1, 'Veggies': 1, 'HvyAlcoholConsump': 0, 'PhysActivity': 0, 'Education': 4, \n",
    "        'Income': 3\n",
    "    }   # Patient ID 0. Taken from the dataset. Probably not used in test_set? - ToDo\n",
    "]\n",
    "\n",
    "# Convert risky patients to pandas dataFrame as it is the expected input format by the RF model\n",
    "risky_df = pd.DataFrame(risky_patients)\n",
    "\n",
    "risky_df = risky_df[X_train.columns]\n",
    "\n",
    "# Predict confidence scores for risky patients\n",
    "risky_confidences = rf_model.predict_proba(risky_df).max(axis=1)\n",
    "\n",
    "# Determine membership status using the same attack threshold from the standard MIAttack\n",
    "risky_membership = ['Member' if c > threshold else 'Non-Member' for c in risky_confidences]\n",
    "\n",
    "for i, (conf, membership) in enumerate(zip(risky_confidences, risky_membership)):\n",
    "    print(f\"Risky Patient {i + 1}: Confidence = {conf:.2f}, Predicted Membership = {membership}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(train_confidences, bins=50, alpha=0.6, color='blue', label=\"Train (Member)\")\n",
    "plt.hist(test_confidences, bins=50, alpha=0.6, color='red', label=\"Test (Non-Member)\")\n",
    "\n",
    "for i, conf in enumerate(risky_confidences):\n",
    "    plt.axvline(conf, color='green', linestyle='dashed', label=f\"Risky Patient {i + 1}: {conf:.2f}\")\n",
    "\n",
    "plt.axvline(threshold, color='black', linestyle='solid', label=f\"Attack Threshold: {threshold:.2f}\")\n",
    "plt.xlabel(\"Model Confidence Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Membership Inference Attack - Risky Patients\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add description, explain results and describe and how it can relate to a real world case (like an insurance company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.2.2 Attribute Inference Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "assert 'rf_model' in locals(), \"Random Forest model (rf_model) is not defined!\" #if not defined, run 6.6\n",
    "\n",
    "sensitive_feature = 'Income'\n",
    "\n",
    "# Exclude 'Income' from training features - because we will be trying to infer this attribute\n",
    "attack_features = [f for f in X_train.columns if f != sensitive_feature]\n",
    "\n",
    "# Get confidence scores from the target model\n",
    "# For each prediction, predict_proba() gives an array of probabilities for each class (8 in case of Income)\n",
    "train_confidences = rf_model.predict_proba(X_train).max(axis=1)\n",
    "test_confidences = rf_model.predict_proba(X_test).max(axis=1)\n",
    "\n",
    "# Add confidence scores to the dataset\n",
    "X_train_attack = X_train[attack_features].copy()\n",
    "X_train_attack['model_confidence'] = train_confidences\n",
    "\n",
    "X_test_attack = X_test[attack_features].copy()\n",
    "X_test_attack['model_confidence'] = test_confidences\n",
    "\n",
    "\n",
    "y_train_attack = X_train[sensitive_feature]\n",
    "y_test_attack = X_test[sensitive_feature]\n",
    "\n",
    "# Train the Inference Model (using random forest)\n",
    "inference_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)\n",
    "inference_model.fit(X_train_attack, y_train_attack)\n",
    "\n",
    "# Evaluate the Inference Model\n",
    "y_pred_attack = inference_model.predict(X_test_attack)\n",
    "\n",
    "attack_accuracy = accuracy_score(y_test_attack, y_pred_attack)\n",
    "\n",
    "train_mean_conf = np.mean(train_confidences)\n",
    "test_mean_conf = np.mean(test_confidences)\n",
    "\n",
    "print(\"\\nAttribute Inference Attack on 'Income':\")\n",
    "print(f\"Attack Accuracy: {attack_accuracy:.2f}\")\n",
    "print(f\"Avg Train Confidence: {train_mean_conf:.2f}\")\n",
    "print(f\"Avg Test Confidence: {test_mean_conf:.2f}\\n\")\n",
    "\n",
    "report = classification_report(y_test_attack, y_pred_attack, output_dict=True)\n",
    "\n",
    "def print_classification_report(report):\n",
    "    headers = [\"Metric\"] + [str(label) for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "    rows = [\n",
    "        [\"Precision\"] + [report[label]['precision'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "        [\"Recall\"] + [report[label]['recall'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "        [\"F1-Score\"] + [report[label]['f1-score'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "        [\"Support\"] + [report[label]['support'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "    ]\n",
    "\n",
    "    # Add macro and weighted averages\n",
    "    rows.append([\"Macro Avg\", report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score'], report['macro avg']['support']])\n",
    "    rows.append([\"Weighted Avg\", report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score'], report['weighted avg']['support']])\n",
    "    rows.append([\"Accuracy\", \"\", \"\", report['accuracy'], \"\"])\n",
    "\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\", floatfmt=\".2f\"))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.hist(train_confidences, bins=50, alpha=0.6, label=\"Train (Member)\", color='blue')\n",
    "plt.hist(test_confidences, bins=50, alpha=0.6, label=\"Test (Non-Member)\", color='red')\n",
    "plt.axvline(train_mean_conf, color='blue', linestyle='dashed', linewidth=1, label=f\"Mean Train Confidence: {train_mean_conf:.2f}\")\n",
    "plt.axvline(test_mean_conf, color='red', linestyle='dashed', linewidth=1, label=f\"Mean Test Confidence: {test_mean_conf:.2f}\")\n",
    "plt.xlabel(\"Model Confidence Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Attribute Inference Attack - Confidence Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attack achieved 47% accuracy, meaning it correctly inferred the income level of individuals nearly half the time. As the Income attribute has 8 levels, it is normal that the accuracy is lower. If the attack tried to guess randomly, it would be around 12,5% (1/8 *100). Therefore data is leaking. \n",
    "\n",
    "Also, the model showed higher confidence on training data (0.91) than test data (0.83), indicating a potential privacy risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.2.3 Find most exposed individuals and the analyzing the infered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Add predictions and confidence to the test dataset for analysis\n",
    "vulnerable_data = X_test_attack.copy()\n",
    "vulnerable_data['True_Income'] = y_test_attack.values\n",
    "vulnerable_data['Predicted_Income'] = y_pred_attack\n",
    "\n",
    "# Calculate prediction correctness\n",
    "vulnerable_data['Correct_Prediction'] = (vulnerable_data['True_Income'] == vulnerable_data['Predicted_Income'])\n",
    "\n",
    "# Fidn top 20 most vulnerable individuals (highest confidence)\n",
    "most_vulnerable = vulnerable_data.sort_values(by='model_confidence', ascending=False).head(20)\n",
    "\n",
    "print(\"Most Vulnerable Individuals (Top 20 by Confidence Score):\")\n",
    "print(tabulate(most_vulnerable[['model_confidence', 'True_Income', 'Predicted_Income', 'Correct_Prediction']],\n",
    "               headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 most vulnerable individuals had a model confidence of 1.0, meaning the model was completely certain about their predicted income. 16 out of 20 predictions were completely correct (80%), highlighting a significant privacy concern for these individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Applying Differential Privacy Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Model Agnostic Private Learning (MAPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Implementing the functions for applying MAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing function that adds noise to prediction probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add noise to prediction probabilities (output perturbation)\n",
    "def add_laplace_noise(predictions, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Add Laplace noise to predictions to achieve differential privacy\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: original prediction probabilities\n",
    "    - epsilon: privacy parameter (lower = more privacy but less accuracy)\n",
    "    \n",
    "    Returns:\n",
    "    - noisy_predictions: predictions with Laplace noise added\n",
    "    \"\"\"\n",
    "    # Scale parameter for Laplace distribution \n",
    "    sensitivity = 1.0  # Maximum change in prediction when one training example changes\n",
    "    scale = sensitivity / epsilon  \n",
    "    \n",
    "    # Generate Laplace noise\n",
    "    noise = np.random.laplace(0, scale, predictions.shape)\n",
    "    \n",
    "    # Add noise to predictions\n",
    "    noisy_predictions = predictions + noise\n",
    "    \n",
    "    # Clip values to be between 0 and 1\n",
    "    noisy_predictions = np.clip(noisy_predictions, 0, 1)\n",
    "    \n",
    "    return noisy_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing function for making differentially private predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prediction-based MAPL\n",
    "def mapl_predict(model, X, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Make differentially private predictions using MAPL\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained ML model with predict_proba method\n",
    "    - X: features for prediction\n",
    "    - epsilon: privacy parameter\n",
    "    \n",
    "    Returns:\n",
    "    - Private binary predictions\n",
    "    \"\"\"\n",
    "    # Get original probability predictions\n",
    "    probabilities = model.predict_proba(X)\n",
    "    \n",
    "    # Add Laplace noise to the probabilities\n",
    "    private_probabilities = add_laplace_noise(probabilities, epsilon)\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    private_predictions = (private_probabilities[:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "    return private_predictions, private_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing function for evaluating model with different privacy levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models with different privacy levels\n",
    "def evaluate_privacy_utility_tradeoff(model, X_test, y_test, epsilons=[0.01, 0.1, 0.5, 1.0, 10.0, 100.0]):\n",
    "    \"\"\"\n",
    "    Evaluate model performance across different privacy levels\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained ML model\n",
    "    - X_test: test features\n",
    "    - y_test: test labels\n",
    "    - epsilons: list of privacy parameters to test\n",
    "    \n",
    "    Returns:\n",
    "    - results: dataframe with evaluation metrics for each privacy level\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get non-private baseline performance\n",
    "    baseline_pred = model.predict(X_test)\n",
    "    baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "    baseline_probas = model.predict_proba(X_test)[:, 1]\n",
    "    baseline_auc = roc_auc_score(y_test, baseline_probas)\n",
    "    \n",
    "    results.append({\n",
    "        'epsilon': 'Baseline (No Privacy)',\n",
    "        'accuracy': baseline_accuracy,\n",
    "        'auc': baseline_auc,\n",
    "        'privacy_level': 'None'\n",
    "    })\n",
    "    \n",
    "    # Evaluate performance at different privacy levels\n",
    "    for epsilon in epsilons:\n",
    "        private_pred, private_probas = mapl_predict(model, X_test, epsilon)\n",
    "        accuracy = accuracy_score(y_test, private_pred)\n",
    "        auc = roc_auc_score(y_test, private_probas[:, 1])\n",
    "        \n",
    "        # Determine privacy level label\n",
    "        if epsilon < 0.1:\n",
    "            privacy_level = 'Very High'\n",
    "        elif epsilon < 1.0:\n",
    "            privacy_level = 'High'\n",
    "        elif epsilon < 10.0:\n",
    "            privacy_level = 'Medium'\n",
    "        else:\n",
    "            privacy_level = 'Low'\n",
    "            \n",
    "        results.append({\n",
    "            'epsilon': epsilon,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'privacy_level': privacy_level\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing function for training an ensemble of models for more robust predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement MAPL with ensemble approach (more robust to noise)\n",
    "def train_mapl_ensemble(X_train, y_train, n_models=10, sample_fraction=0.8, \n",
    "                        base_model=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                        epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Train an ensemble of models for more robust private predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: training features\n",
    "    - y_train: training labels\n",
    "    - n_models: number of models in ensemble\n",
    "    - sample_fraction: fraction of data to sample for each model\n",
    "    - base_model: base model to clone for ensemble\n",
    "    - epsilon: privacy parameter for final predictions\n",
    "    \n",
    "    Returns:\n",
    "    - ensemble: list of trained models\n",
    "    - epsilon: privacy parameter used\n",
    "    \"\"\"\n",
    "    ensemble = []\n",
    "    n_samples = int(len(X_train) * sample_fraction)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        # Sample data with replacement\n",
    "        indices = np.random.choice(len(X_train), n_samples, replace=True)\n",
    "        X_sample = X_train.iloc[indices]\n",
    "        y_sample = y_train.iloc[indices]\n",
    "        \n",
    "        # Clone and train model\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_sample, y_sample)\n",
    "        ensemble.append(model)\n",
    "    \n",
    "    return ensemble, epsilon\n",
    "\n",
    "def predict_with_mapl_ensemble(ensemble, X, epsilon):\n",
    "    \"\"\"\n",
    "    Make differentially private predictions using an ensemble\n",
    "    \n",
    "    Parameters:\n",
    "    - ensemble: list of trained models\n",
    "    - X: features for prediction\n",
    "    - epsilon: privacy parameter\n",
    "    \n",
    "    Returns:\n",
    "    - Private predictions\n",
    "    \"\"\"\n",
    "    # Get predictions from each model\n",
    "    all_probas = np.zeros((len(X), 2))\n",
    "    \n",
    "    # Divide epsilon budget among models\n",
    "    model_epsilon = epsilon / len(ensemble)\n",
    "    \n",
    "    for model in ensemble:\n",
    "        # Get probability predictions\n",
    "        probas = model.predict_proba(X)\n",
    "        \n",
    "        # Add noise to each model's predictions\n",
    "        noisy_probas = add_laplace_noise(probas, model_epsilon)\n",
    "        \n",
    "        # Accumulate predictions\n",
    "        all_probas += noisy_probas\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_probas = all_probas / len(ensemble)\n",
    "    \n",
    "    # Make final binary predictions\n",
    "    predictions = (avg_probas[:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "    return predictions, avg_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Applying MAPL to the Balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the functions we defined in 7.1.1 to apply MAPL to the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "\n",
    "# First, we need to ensure we're using the exact same features that were used to train the model\n",
    "# Extract the features used in the original training\n",
    "print(\"Getting the feature names used in the original model...\")\n",
    "if hasattr(rf_model, 'feature_names_in_'):\n",
    "    original_features = rf_model.feature_names_in_\n",
    "    print(f\"Original model was trained with these features: {original_features}\")\n",
    "else:\n",
    "    # Important features we used earlier\n",
    "    original_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                       'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                       'CholCheck', 'Smoker', 'NoDocbcCost', 'Sex', 'AnyHealthcare', \n",
    "                       'Income', 'Education']\n",
    "    print(f\"Using features from section 6.4: {original_features}\")\n",
    "\n",
    "# Ensure test data has the same features in same order\n",
    "X_test_for_mapl = X_test_bal[original_features].copy()\n",
    "X_train_for_mapl = X_train_bal[original_features].copy()\n",
    "\n",
    "# Use the original model\n",
    "print(\"Using the existing Random Forest model from section 6.4\")\n",
    "print(f\"Original Balanced Model Accuracy: {accuracy_score(y_test_bal, y_pred_bal):.4f}\")\n",
    "print(\"Original Classification Report (from section 6.4):\")\n",
    "print(classification_report(y_test_bal, y_pred_bal))\n",
    "\n",
    "# Apply MAPL output perturbation with different privacy levels\n",
    "print(\"\\n\\nEvaluating privacy-utility tradeoff with MAPL...\")\n",
    "epsilon_values = [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0]\n",
    "privacy_results = evaluate_privacy_utility_tradeoff(rf_model,  # Using existing rf_model from section 6.4\n",
    "                                                  X_test_for_mapl,  # Using the correctly filtered features \n",
    "                                                  y_test_bal, \n",
    "                                                  epsilons=epsilon_values)\n",
    "\n",
    "# Display privacy-utility tradeoff\n",
    "print(\"\\nPrivacy-Utility Tradeoff Results:\")\n",
    "print(privacy_results)\n",
    "\n",
    "# Visualize privacy-utility tradeoff\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot only the values with numeric epsilon (exclude baseline)\n",
    "numeric_results = privacy_results[privacy_results['epsilon'] != 'Baseline (No Privacy)'].copy()\n",
    "numeric_results['epsilon'] = numeric_results['epsilon'].astype(float)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(numeric_results['epsilon'], numeric_results['accuracy'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xscale('log')  # Log scale for epsilon\n",
    "plt.xlabel('Privacy Parameter (ε) - Lower is More Private', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.title('Accuracy vs. Privacy', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(numeric_results['epsilon'], numeric_results['auc'], 'o-', linewidth=2, color='green', markersize=8)\n",
    "plt.xscale('log')  # Log scale for epsilon\n",
    "plt.xlabel('Privacy Parameter (ε) - Lower is More Private', fontsize=10)\n",
    "plt.ylabel('AUC Score', fontsize=10)\n",
    "plt.title('AUC vs. Privacy', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train and apply MAPL ensemble for more robust results\n",
    "print(\"\\n\\nTraining MAPL ensemble model...\")\n",
    "epsilon = 1.0  # Choose a reasonable privacy level\n",
    "# Use the filtered features for training ensemble\n",
    "ensemble, ensemble_epsilon = train_mapl_ensemble(X_train_for_mapl, y_train_bal, \n",
    "                                               n_models=10, \n",
    "                                               sample_fraction=0.8,\n",
    "                                               base_model=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                                               epsilon=epsilon)\n",
    "\n",
    "# Make private predictions with the ensemble\n",
    "ensemble_pred, ensemble_probas = predict_with_mapl_ensemble(ensemble, X_test_for_mapl, epsilon)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test_bal, ensemble_pred)\n",
    "print(f\"\\nMAPL Ensemble Model Accuracy (ε={epsilon}): {ensemble_accuracy:.4f}\")\n",
    "print(\"\\nMAPL Ensemble Model Classification Report:\")\n",
    "print(classification_report(y_test_bal, ensemble_pred))\n",
    "\n",
    "# Compare ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Standard model curve (using existing rf_model)\n",
    "standard_probas = rf_model.predict_proba(X_test_for_mapl)[:, 1]\n",
    "fpr_standard, tpr_standard, _ = roc_curve(y_test_bal, standard_probas)\n",
    "roc_auc_standard = auc(fpr_standard, tpr_standard)\n",
    "plt.plot(fpr_standard, tpr_standard, \n",
    "         label=f'Standard Model (AUC = {roc_auc_standard:.3f})',\n",
    "         linewidth=2)\n",
    "\n",
    "# MAPL with direct perturbation\n",
    "_, mapl_direct_probas = mapl_predict(rf_model, X_test_for_mapl, epsilon)\n",
    "fpr_mapl, tpr_mapl, _ = roc_curve(y_test_bal, mapl_direct_probas[:, 1])\n",
    "roc_auc_mapl = auc(fpr_mapl, tpr_mapl)\n",
    "plt.plot(fpr_mapl, tpr_mapl, \n",
    "         label=f'MAPL Direct (ε={epsilon}, AUC = {roc_auc_mapl:.3f})',\n",
    "         linewidth=2)\n",
    "\n",
    "# MAPL ensemble\n",
    "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test_bal, ensemble_probas[:, 1])\n",
    "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
    "plt.plot(fpr_ensemble, tpr_ensemble, \n",
    "         label=f'MAPL Ensemble (ε={epsilon}, AUC = {roc_auc_ensemble:.3f})',\n",
    "         linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison: Standard vs. MAPL Methods')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Generate feature importance analysis with privacy considerations\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    # Get feature importances from existing model\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Add noise to feature importances for privacy\n",
    "    def privatize_importances(importances, epsilon=1.0):\n",
    "        sensitivity = 1.0/len(importances)  # Maximum impact of one sample on importances\n",
    "        scale = sensitivity/epsilon\n",
    "        noise = np.random.laplace(0, scale, size=len(importances))\n",
    "        noisy_importances = importances + noise\n",
    "        # Ensure all importances are non-negative\n",
    "        noisy_importances = np.maximum(0, noisy_importances)\n",
    "        # Re-normalize to sum to 1\n",
    "        noisy_importances = noisy_importances / noisy_importances.sum()\n",
    "        return noisy_importances\n",
    "    \n",
    "    # Get private feature importances\n",
    "    private_importances = privatize_importances(importances, epsilon)\n",
    "    \n",
    "    # Get feature names\n",
    "    if hasattr(rf_model, 'feature_names_in_'):\n",
    "        features = rf_model.feature_names_in_\n",
    "    else:\n",
    "        features = original_features\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Standard Importance': importances,\n",
    "        'Private Importance': private_importances\n",
    "    })\n",
    "    \n",
    "    # Sort by standard importance\n",
    "    importance_df = importance_df.sort_values('Standard Importance', ascending=False)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot standard vs private importances\n",
    "    ind = np.arange(len(importance_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.barh(ind + width/2, importance_df['Standard Importance'], width, label='Standard Importance', color='blue', alpha=0.7)\n",
    "    plt.barh(ind - width/2, importance_df['Private Importance'], width, label='Private Importance', color='red', alpha=0.7)\n",
    "    \n",
    "    plt.yticks(ind, importance_df['Feature'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title('Feature Importance: Standard vs. Private (ε={})'.format(epsilon))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation & Performance Metrics with MAPL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluation & Performance Metrics with MAPL\n",
    "print(\"\\n================= MAPL EVALUATION SUMMARY =================\")\n",
    "print(\"\\nPrivacy-Utility Tradeoff Summary:\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"| Privacy Level | Epsilon | Accuracy | AUC  | Privacy Impact |\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "for _, row in privacy_results.iterrows():\n",
    "    if row['epsilon'] == 'Baseline (No Privacy)':\n",
    "        print(f\"| {row['privacy_level']:<13} | {'N/A':<7} | {row['accuracy']:.4f} | {row['auc']:.4f} | None            |\")\n",
    "    else:\n",
    "        # Calculate privacy impact as percentage decrease from baseline\n",
    "        baseline = privacy_results[privacy_results['epsilon'] == 'Baseline (No Privacy)']\n",
    "        accuracy_impact = (baseline['accuracy'].values[0] - row['accuracy']) / baseline['accuracy'].values[0] * 100\n",
    "        print(f\"| {row['privacy_level']:<13} | {row['epsilon']:<7} | {row['accuracy']:.4f} | {row['auc']:.4f} | {accuracy_impact:.2f}% decrease |\")\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "# Calculate average performance metrics\n",
    "avg_accuracy_high_privacy = privacy_results[\n",
    "    (privacy_results['privacy_level'] == 'High') | \n",
    "    (privacy_results['privacy_level'] == 'Very High')\n",
    "]['accuracy'].mean()\n",
    "\n",
    "avg_accuracy_med_low_privacy = privacy_results[\n",
    "    (privacy_results['privacy_level'] == 'Medium') | \n",
    "    (privacy_results['privacy_level'] == 'Low')\n",
    "]['accuracy'].mean()\n",
    "\n",
    "print(f\"Average accuracy with high privacy (ε < 1.0): {avg_accuracy_high_privacy:.4f}\")\n",
    "print(f\"Average accuracy with medium/low privacy (ε ≥ 1.0): {avg_accuracy_med_low_privacy:.4f}\")\n",
    "\n",
    "# Find best privacy-utility tradeoff\n",
    "numeric_results = privacy_results[privacy_results['epsilon'] != 'Baseline (No Privacy)'].copy()\n",
    "if not numeric_results.empty:\n",
    "    numeric_results['epsilon'] = numeric_results['epsilon'].astype(float)\n",
    "    \n",
    "    # Calculate a combined score that balances privacy and utility\n",
    "    # Lower epsilon is better for privacy, higher accuracy is better for utility\n",
    "    numeric_results['privacy_score'] = 1 / numeric_results['epsilon']\n",
    "    max_privacy = numeric_results['privacy_score'].max()\n",
    "    max_accuracy = numeric_results['accuracy'].max()\n",
    "    \n",
    "    # Normalize scores to [0,1] range\n",
    "    numeric_results['norm_privacy'] = numeric_results['privacy_score'] / max_privacy\n",
    "    numeric_results['norm_accuracy'] = numeric_results['accuracy'] / max_accuracy\n",
    "    \n",
    "    # Combined score (equal weighting)\n",
    "    numeric_results['combined_score'] = (numeric_results['norm_privacy'] + numeric_results['norm_accuracy']) / 2\n",
    "    \n",
    "    # Find best tradeoff\n",
    "    best_tradeoff = numeric_results.loc[numeric_results['combined_score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nRecommended privacy parameter (best tradeoff): ε = {best_tradeoff['epsilon']}\")\n",
    "    print(f\"- Accuracy: {best_tradeoff['accuracy']:.4f}\")\n",
    "    print(f\"- Privacy level: {best_tradeoff['privacy_level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import opacus\n",
    "import numpy\n",
    "\n",
    "print(numpy.__version__) # min 1.21\n",
    "print(opacus.__version__) # min 1.3\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_important)\n",
    "X_test_scaled = scaler.transform(X_test_important)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_bal.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_bal.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = FeedForwardNN(input_dim=X_train_tensor.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "privacy_engine = opacus.PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    target_epsilon=5.0,     # Set desired privacy budget\n",
    "    target_delta=1e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    acc = (predicted_classes == y_test_tensor).float().mean().item()\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "print(f\"Final ε: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion & Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
