{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Balancing Privacy and Accuracy in Machine Learning Models with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Introduction & Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores how **Differential Privacy Stochastic Gradient Descent (DP-SGD)** and **Model Agnostic Private Learning (MAPL)** impact machine learning models. Specifically, we examine whether DP techniques interfere with model accuracy and the evolution of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Setup - Install Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if there are missing packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn==1.3.2\n",
    "# %pip install ucimlrepo\n",
    "# %pip install imblearn\n",
    "# %pip install tabulate\n",
    "# %pip install boruta\n",
    "# %pip install torch\n",
    "# %pip install opacus\n",
    "# %pip install scipy\n",
    "# %pip install numpy\n",
    "# %pip install imbalanced-learn==0.11.0\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "required = {\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"pandas\",\n",
    "    \"ucimlrepo\",\n",
    "    \"tabulate\",\n",
    "    \"boruta\",\n",
    "    \"torch\",\n",
    "    \"opacus\",\n",
    "    \"scipy\",\n",
    "    \"numpy==1.26.4\",\n",
    "    \"tqdm\",\n",
    "}\n",
    "\n",
    "installed = {pkg for pkg in required if importlib.util.find_spec(pkg.split('=')[0]) is not None}\n",
    "# Special handling for scikit-learn as it may be installed under the name 'sklearn'\n",
    "if importlib.util.find_spec(\"sklearn\") is not None:\n",
    "    installed.add(\"scikit-learn\")\n",
    "# Special handling for imbalanced-learn as it is installed under the name 'imblearn'\n",
    "if importlib.util.find_spec(\"imblearn\") is not None:\n",
    "    installed.add(\"imbalanced-learn\")\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    print(f\"The following libraries are missing: {', '.join(missing)}\")\n",
    "    print(\"Attempting to install missing libraries...\")\n",
    "    %pip install {\" \".join(missing)}\n",
    "    print(\"Libraries installed successfully.\")\n",
    "else:\n",
    "    print(\"All required libraries are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_file(data, file_path, params={}):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        print(f\"Creating directory: {os.path.dirname(file_path)}\")\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    if file_path.endswith('.csv'):\n",
    "        data.to_csv(file_path, index=False, **params)\n",
    "    elif file_path.endswith('.png') or file_path.endswith('.jpg'):\n",
    "        data.savefig(file_path)\n",
    "    elif file_path.endswith('.pkl'):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "results_gInfo_dir = \"results/generalInfo\"\n",
    "results_fsData_dir = \"results/featureSelectionData\"\n",
    "results_models_dir = \"results/models\"\n",
    "models_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 . Data Exploration & Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing the relevant libraries and getting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo #to retrieve the dataset\n",
    "\n",
    "# Getting the data\n",
    "dataset_id = 891  # our chosen dataset\n",
    "dataset_name = \"dataset.csv\"\n",
    "downloaded = os.path.isfile(dataset_name)\n",
    "df =  pd.read_csv(dataset_name) if downloaded else fetch_ucirepo(id=dataset_id).data.original\n",
    "\n",
    "# # Previous way to retrieve the dataset (for documentation)\n",
    "# url = 'https://archive.ics.uci.edu/static/public/891/data.csv'\n",
    "# df = pd.read_csv(url)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "if not downloaded:\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    save_file(df, \"./\" + dataset_name)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the dataset contains 253,680 records with 23 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking basic Dataset Information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that all features are stored as integer data types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the features and their descriptions:\n",
    "\n",
    "| Variable Name     | Role    | Type    | Description                                                                 |\n",
    "|-------------------|---------|---------|-----------------------------------------------------------------------------|\n",
    "| ID                | ID      | Integer | Patient ID                                                                  |\n",
    "| Diabetes_binary   | Target  | Binary  | 0 = no diabetes 1 = prediabetes or diabetes                                 |\n",
    "| HighBP            | Feature | Binary  | 0 = no high BP 1 = high BP                                                  |\n",
    "| HighChol          | Feature | Binary  | 0 = no high cholesterol 1 = high cholesterol                                |\n",
    "| CholCheck         | Feature | Binary  | 0 = no cholesterol check in 5 years 1 = yes cholesterol check in 5 years    |\n",
    "| BMI               | Feature | Integer | Body Mass Index                                                             |\n",
    "| Smoker            | Feature | Binary  | Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] 0 = no 1 = yes |\n",
    "| Stroke            | Feature | Binary  | (Ever told) you had a stroke. 0 = no 1 = yes                                |\n",
    "| HeartDiseaseorAttack | Feature | Binary | coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes  |\n",
    "| PhysActivity      | Feature | Binary  | physical activity in past 30 days - not including job 0 = no 1 = yes        |\n",
    "| Fruits            | Feature | Binary  | Consume Fruit 1 or more times per day 0 = no 1 = yes                        |\n",
    "| Veggies           | Feature | Binary  | Consume Vegetables 1 or more times per day 0 = no 1 = yes                   |\n",
    "| HvyAlcoholConsump | Feature | Binary  | Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) 0 = no 1 = yes |\n",
    "| AnyHealthcare     | Feature | Binary  | Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no 1 = yes |\n",
    "| NoDocbcCost       | Feature | Binary  | Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes |\n",
    "| GenHlth           | Feature | Integer | General health: 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor |\n",
    "| MentHlth          | Feature | Integer | For how many days during the past 30 days was your mental health not good? scale 1-30 days |\n",
    "| PhysHlth          | Feature | Integer | For how many days during the past 30 days was your physical health not good? scale 1-30 days |\n",
    "| DiffWalk          | Feature | Binary  | Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes   |\n",
    "| Sex               | Feature | Binary  | 0 = female 1 = male                                                    |\n",
    "| Age               | Feature | Integer | 13 levels, 1 = 18-24, 9 = 60-64, 13 = 80 or older              |\n",
    "| Education         | Feature | Integer | 6 levels, 1 = Never attended school or only kindergarten, 2 = Grades 1 through 8, 3 = Grades 9 through 11, 4 = Grade 12 or GED, 5 = College 1 year to 3 years, 6 = College 4 years or more |\n",
    "| Income            | Feature | Integer | 8 levels, 1 = less than $10,000, 5 = less than $35,000, 8 = $75,000 or more |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed basic statistics for the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Save basic statistics to a CSV file\n",
    "save_file(df.describe(), os.path.join(results_gInfo_dir, \"basic_statistics.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that there are no missing values (as stated in the dataset description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the dataset only has 13.93% records with diabetes. We will address this imbalance later during data cleaning and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of dataset\n",
    "diabetes_counts = df['Diabetes_binary'].value_counts()\n",
    "print(\"Distribution of target variable:\")\n",
    "print(diabetes_counts)\n",
    "print(f\"Percentage of records with diabetes: {diabetes_counts[1]/len(df)*100:.2f}%\")\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "diabetes_counts.plot(kind='bar', color=['skyblue', 'orange'])\n",
    "plt.title(\"Distribution of Target Variable (Diabetes_binary)\")\n",
    "plt.xlabel(\"Diabetes_binary\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(ticks=[0, 1], labels=[f\"No Diabetes ({diabetes_counts[0]:,})\", f\"Diabetes ({diabetes_counts[1]:,})\"], rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = os.path.join(results_gInfo_dir, \"imbalanced_diabetes_distribution.png\")\n",
    "save_file(plt, plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 . Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in preprocessing we would handle missing values, encoding binary data (True &rarr; 1), encoding categorical data, feature scaling. However, since the dataset is already clean and preprocessed, we will only address the class imbalance issue.\n",
    "In the balanced datased, we have reduced the number of records with diabetes to match the number of records without diabetes, so that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_with_diabetes = df['Diabetes_binary'].value_counts()[1]\n",
    "\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "print(f\"- Total samples in the original dataset: {len(df)}\")\n",
    "print(f\"- Samples with diabetes (class 1): {num_with_diabetes}\")\n",
    "print(f\"- Samples without diabetes (class 0): {df['Diabetes_binary'].value_counts()[0]}\")\n",
    "\n",
    "df_no_diabetes = df[df['Diabetes_binary'] == 0].sample(n=num_with_diabetes, random_state=42)\n",
    "df_with_diabetes = df[df['Diabetes_binary'] == 1]\n",
    "\n",
    "df_balanced = pd.concat([df_no_diabetes, df_with_diabetes])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_balanced_noTarget = df_balanced.drop(columns=[\"Diabetes_binary\"])\n",
    "y_balanced = df_balanced[\"Diabetes_binary\"]\n",
    "\n",
    "print(\"\\nBalanced Dataset:\")\n",
    "print(f\"- Total samples in the balanced dataset: {len(df_balanced)}\")\n",
    "print(df_balanced['Diabetes_binary'].value_counts())\n",
    "\n",
    "print(f\"- Samples without diabetes (class 0): {df_balanced['Diabetes_binary'].value_counts()[0]}\")\n",
    "print(f\"- Samples with diabetes (class 1): {df_balanced['Diabetes_binary'].value_counts()[1]}\")\n",
    "\n",
    "# Plot the distribution of the balanced dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_balanced['Diabetes_binary'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])\n",
    "plt.title(\"Distribution of Target Variable (Balanced Dataset)\")\n",
    "plt.xlabel(\"Diabetes_binary\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"No Diabetes\", \"Diabetes\"], rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_path_balanced = os.path.join(results_gInfo_dir, \"balanced_diabetes_distribution.png\")\n",
    "save_file(plt, plot_path_balanced)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Splitting Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our models we have to split the data into a training set and a test set. We have chosen a 80/20 split, which ensures we have enough training data for the model to learn patterns and enough data for performance evaluation. We have used stratified sampling to ensure that both the training set and test set includes a 50/50 split of records having diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_balanced = df_balanced.drop('Diabetes_binary', axis=1) # drop the target column\n",
    "y_balanced = df_balanced['Diabetes_binary']\n",
    "\n",
    "X_imbalanced = df.drop('Diabetes_binary', axis=1) #drop the target column\n",
    "y_imbalanced = df['Diabetes_binary']\n",
    "\n",
    "# split into train (70%) and temp (30%)\n",
    "X_train_bal, X_temp_bal, y_train_bal, y_temp_bal = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# split temp set into validation (10%) and test (20%)\n",
    "X_val_bal, X_test_bal, y_val_bal, y_test_bal = train_test_split(\n",
    "    X_temp_bal, y_temp_bal, test_size=2/3, random_state=42, stratify=y_temp_bal\n",
    ")\n",
    "\n",
    "# Imbalanced dataset\n",
    "X_train_imb, X_temp_imb, y_train_imb, y_temp_imb = train_test_split(\n",
    "    X_imbalanced, y_imbalanced, test_size=0.3, random_state=42, stratify=y_imbalanced\n",
    ")\n",
    "\n",
    "X_val_imb, X_test_imb, y_val_imb, y_test_imb = train_test_split(\n",
    "    X_temp_imb, y_temp_imb, test_size=2/3, random_state=42, stratify=y_temp_imb\n",
    ")\n",
    "\n",
    "print(f\"Balanced Training set: {X_train_bal.shape[0]} samples\")\n",
    "print(f\"Balanced Validation set: {X_val_bal.shape[0]} samples\")\n",
    "print(f\"Balanced Testing set: {X_test_bal.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in balanced training set:\")\n",
    "print(y_train_bal.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in balanced validation set:\")\n",
    "print(y_val_bal.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in balanced testing set:\")\n",
    "print(y_test_bal.value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nImbalanced Training set: {X_train_imb.shape[0]} samples\")\n",
    "print(f\"Imbalanced Validation set: {X_val_imb.shape[0]} samples\")\n",
    "print(f\"Imbalanced Testing set: {X_test_imb.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced training set:\")\n",
    "print(y_train_imb.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced validation set:\")\n",
    "print(y_val_imb.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in imbalanced testing set:\")\n",
    "print(y_test_imb.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be a defense against the attacks, because the less features we have, the less information we leak.\n",
    "\n",
    "We need to explain which features we have chosen based on the data exploration above and the following feature selection methods. We can use variance threshold, correlation matrix, kbest, rfe, boruta...\n",
    "\n",
    "['HighBP', 'HighChol', 'BMI', 'GenHlth', 'Age'] ok with V.T. and C.M.\n",
    "\n",
    "Might want to use more features to try the attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    RFE,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from boruta import BorutaPy\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, classification_report\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "def save_results(results, file_path, columns):\n",
    "    results_df = pd.DataFrame(results, columns=columns)\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return (accuracy, precision, recall, f1)\n",
    "\n",
    "def getVariances(threshold, data):\n",
    "    var_threshold = VarianceThreshold(threshold=threshold)\n",
    "    var_threshold.fit(data)\n",
    "    variances = var_threshold.variances_\n",
    "    # Create a DataFrame for variances\n",
    "    variances_df = pd.DataFrame(variances, index=data.columns, columns=[\"Variance\"])\n",
    "    # Sort variances in descending order\n",
    "    variances_df = variances_df.sort_values(by=\"Variance\", ascending=False)\n",
    "    # Features with variance >= threshold\n",
    "    features_high_variance = variances_df[variances_df[\"Variance\"] >= threshold]\n",
    "    # Features with variance < threshold\n",
    "    features_low_variance = variances_df[variances_df[\"Variance\"] < threshold]\n",
    "    return variances_df, features_high_variance, features_low_variance\n",
    "\n",
    "def getKBest(k, data, target):\n",
    "    # selection\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    x_kbest = selector.fit_transform(data, target)\n",
    "    # Create a DataFrame with the selected features\n",
    "    kbest_features = data.columns[selector.get_support()]\n",
    "    x_kbest_df = pd.DataFrame(x_kbest, columns=kbest_features)\n",
    "    return x_kbest_df\n",
    "\n",
    "def process_kbest(data, target, k, file_path):\n",
    "    try:\n",
    "        x_kbest_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        x_kbest_df = getKBest(k, data, target)\n",
    "        x_kbest_df[target.name] = target.values\n",
    "        x_kbest_df.to_csv(file_path, index=False)\n",
    "    x_kbest_df = x_kbest_df.drop(columns=[target.name], axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_kbest_df, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    y_pred = rf_model.predict(x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    kbest_features_names = x_kbest_df.columns.tolist()\n",
    "    return (k, accuracy, precision, recall, f1, kbest_features_names)\n",
    "\n",
    "def getRFE(k, data, target):\n",
    "    # selection\n",
    "    selector = RFE(estimator=rf_model, n_features_to_select=k)\n",
    "    x_rfe = selector.fit_transform(data, target)\n",
    "    # Create a DataFrame with the selected features\n",
    "    rfe_features = data.columns[selector.get_support()]\n",
    "    x_rfe_df = pd.DataFrame(x_rfe, columns=rfe_features)\n",
    "    return x_rfe_df\n",
    "\n",
    "def process_rfe(data, target, k, file_path):\n",
    "    try:\n",
    "        x_rfe_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        x_rfe_df = getRFE(k, data, target)\n",
    "        x_rfe_df[target.name] = target.values\n",
    "        x_rfe_df.to_csv(file_path, index=False)\n",
    "    x_rfe_df = x_rfe_df.drop(columns=[target.name], axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_rfe_df, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    y_pred = rf_model.predict(x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    rfe_features_names = x_rfe_df.columns.tolist()\n",
    "    return (k, accuracy, precision, recall, f1, rfe_features_names)\n",
    "\n",
    "def apply_boruta(data, target):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, target, test_size=0.3, random_state=42, stratify=target\n",
    "    )\n",
    "    boruta = BorutaPy(\n",
    "        rf_model,\n",
    "        n_estimators=\"auto\",\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    boruta.fit(x_train.values, y_train.values)\n",
    "    sel_x_train = boruta.transform(x_train.values)\n",
    "    sel_x_test = boruta.transform(x_test.values)\n",
    "    rf_model.fit(sel_x_train, y_train)\n",
    "    y_pred = rf_model.predict(sel_x_test)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    selected_features = x_train.columns[boruta.support_].tolist()\n",
    "    return (accuracy, precision, recall, f1, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances_df, features_high_variance, features_low_variance = getVariances(\n",
    "    0, df_balanced_noTarget\n",
    ")\n",
    "\n",
    "# Plot variances of all features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(variances_df.index, variances_df['Variance'], color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Feature Variances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Highlight features with high and low variance\n",
    "print(\"\\nFeatures with High Variance:\")\n",
    "print(features_high_variance)\n",
    "print(\"\\nFeatures with Low Variance:\")\n",
    "print(features_low_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_balanced.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on correlations with the target variable\n",
    "diabetes_correlations = correlation_matrix['Diabetes_binary'].sort_values(ascending=False)\n",
    "print(\"\\nFeatures correlated with Diabetes (sorted):\")\n",
    "print(diabetes_correlations)\n",
    "\n",
    "# Visualize correlations with the target\n",
    "plt.figure(figsize=(12, 8))\n",
    "diabetes_correlations[1:].plot(kind='bar')  # Exclude self-correlation\n",
    "plt.title(\"Feature Correlation with Diabetes\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Correlation Coefficient\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# columns = [\"Model\", \"K\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "# for k in range(1, df_balanced_noTarget.shape[1] + 1):\n",
    "#     file_path = f\"featureSelectionData/kbest/k/{k}best_features.csv\"\n",
    "#     kbestResult = process_kbest(df_balanced_noTarget, y_balanced, k, file_path)\n",
    "#     results.append((\"rf\",) + kbestResult)\n",
    "# save_results(\n",
    "#     results, f\"featureSelectionData/kbest/kbest_rf_results.csv\", columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# columns = [\"Model\", \"K\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "# for k in range(1, df_balanced_noTarget.shape[1] + 1):\n",
    "#     file_path = f\"featureSelectionData/rfe/k/{k}rfe_features.csv\"\n",
    "#     rfeResult = process_rfe(df_balanced_noTarget, y_balanced, k, file_path)\n",
    "#     results.append((\"rf\",) + rfeResult)\n",
    "# save_results(results, f\"featureSelectionData/rfe/rfe_rf_results.csv\", columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.5 Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Features\"]\n",
    "# accuracy, precision, recall, f1, selected_features = apply_boruta(\n",
    "#     df_balanced_noTarget, y_balanced\n",
    "# )\n",
    "# save_results(\n",
    "#     [(\"rf\", accuracy, precision, recall, f1, selected_features)],\n",
    "#     f\"featureSelectionData/boruta/boruta_rf_results.csv\",\n",
    "#     columns,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.6 Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_important_features = ['GenHlth', 'HighBP', 'DiffWalk', 'BMI', 'HighChol', 'Age', \n",
    "                      'HeartDiseaseorAttack', 'PhysHlth', 'Stroke', 'MentHlth', \n",
    "                      'CholCheck', 'Smoker', 'NoDocbcCost', 'Sex', 'AnyHealthcare', \n",
    "                      'Income', 'Education']\n",
    "important_features = ['HighBP', 'HighChol', 'BMI', 'GenHlth', 'Age', 'Income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 . Machine Learning Model without Differential Privacy\n",
    "\n",
    "- Precision: How many predicted positives are actually positive?\n",
    "- Recall: How many actual positives were correctly identified?\n",
    "- F1-score: A balance between precision and recall.\n",
    "- Support: Number of actual occurrences of each class.\n",
    "\n",
    "Single-layer networks have just one layer of active units. Inputs connect directly to the outputs through a single layer of weights.\n",
    "\n",
    "In multi-layer networks (MLP) there is a layer of input nodes, a layer of output nodes, and one or more intermediate (hidden) layers.\n",
    "\n",
    "- Input Layer: Input variables, sometimes called the visible layer.\n",
    "- Hidden Layers: Layers of nodes between the input and output layers. There may be one or more of these layers.\n",
    "- Output Layer: A layer of nodes that produce the output variables.\n",
    "- Size: The number of nodes in the model.\n",
    "- Width: The number of nodes in a specific layer.\n",
    "- Depth: The number of layers in a neural network.\n",
    "\n",
    "The structure of an MLP can be summarized using a simple notation: the number of nodes in each layer is specified as an integer, in order from the input layer to the output layer, with the size of each layer separated by a \"/\". For example, a model with 3 input nodes, 2 hidden layers with 4 and 3 nodes respectively, and 1 output node would be represented as \"3/4/3/1\".\n",
    "\n",
    "Steps to train a deep learning model with pytorch:\n",
    "1. Scale the data\n",
    "2. Label encode the target\n",
    "3. Convert the data to tensors\n",
    "4. Create a model\n",
    "   1. Should we use a wide model or a deep model? \n",
    "   2. How many layers and neurons per layer should we use?\n",
    "   3. How to choose? K-fold cross validaton\n",
    "5. Train the model\n",
    "\n",
    "K-fold cross validaton is a technique that, use a “training set” of data to train the model and then use a “test set” of data to see how accurate the model can predict. The result from test set is what we should focus on. But we do not want to test a model once because if we see an extremely good or bad result, it may be by chance. we want to run this process times with different training and test sets, such that we are ensured that we are comparing the model design, not the result of a particular training. K-fold cross validaton splits a larger dataset into portions and take one portion as the test set while the portions are combined as the training set. There are different such combinations. Therefore we can repeat the experiment for times and take the average result. To ensure that each portion contains equal number of classes we will use stratified k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "models_performance_path = os.path.join(results_models_dir, 'models_performance.csv')\n",
    "\n",
    "# Define the model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "# 3 layers model (input, 1 hidden, output)\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(n_features, n_features * 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(n_features * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "# 5 layers model (input, 3 hidden, output)\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_features, n_features)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(n_features, n_features)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(n_features, n_features)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(n_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    " \n",
    "# Compare model sizes (number of nodes)\n",
    "# modelWide = Wide(df_balanced.shape[1])\n",
    "# modelDeep = Deep(df_balanced.shape[1])\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))\n",
    "\n",
    "def get_train_test_tensors(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
    "\n",
    "def getLoader(X_train_tensor, y_train_tensor):\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    return train_loader\n",
    " \n",
    "# Helper function to train one model\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy TODO: OR nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001) # TODO: OR optim.SGD()\n",
    " \n",
    "    n_epochs = 1    # number of epochs to run TODO: how many?\n",
    "    batch_size = 10 # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity TODO: change with precision\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar: # tqdm is a progress bar\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    \n",
    "    return best_acc, model\n",
    "\n",
    "def save_model_performance(classification_report_dict, model_name):\n",
    "    performance = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": classification_report_dict[\"accuracy\"],\n",
    "        \"Precision_0\": classification_report_dict[\"0\"][\"precision\"],\n",
    "        \"Precision_1\": classification_report_dict[\"1\"][\"precision\"],\n",
    "        \"Precision_weighted\": classification_report_dict[\"weighted avg\"][\"precision\"],\n",
    "        \"Recall_0\": classification_report_dict[\"0\"][\"recall\"],\n",
    "        \"Recall_1\": classification_report_dict[\"1\"][\"recall\"],\n",
    "        \"Recall_weighted\": classification_report_dict[\"weighted avg\"][\"recall\"],\n",
    "        \"F1_0\": classification_report_dict[\"0\"][\"f1-score\"],\n",
    "        \"F1_1\": classification_report_dict[\"1\"][\"f1-score\"],\n",
    "        \"F1_weighted\": classification_report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "        \"Support_0\": classification_report_dict[\"0\"][\"support\"],\n",
    "        \"Support_1\": classification_report_dict[\"1\"][\"support\"],\n",
    "        \"Support_weighted\": classification_report_dict[\"weighted avg\"][\"support\"],\n",
    "    }\n",
    "    if os.path.isfile(models_performance_path,):\n",
    "        existing_data = pd.read_csv(models_performance_path)\n",
    "        if performance[\"Model\"] not in existing_data[\"Model\"].values:\n",
    "            save_file(pd.DataFrame([performance]), models_performance_path, {'mode': 'a', 'header': False})\n",
    "        else:\n",
    "            existing_data.loc[existing_data[\"Model\"] == performance[\"Model\"], :] = pd.DataFrame([performance]).values\n",
    "            save_file(existing_data, models_performance_path)\n",
    "    else:\n",
    "        save_file(pd.DataFrame([performance]), models_performance_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Comparing Models with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the data to tensors PyTorch tensors as this is the format a PyTorch model needs\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# n_features = X_train_bal.shape[1]\n",
    "# train_test_tensors_allF_bal = get_train_test_tensors(X_train_bal, y_train_bal, \n",
    "#                                                      X_test_bal, y_test_bal)\n",
    "# X_train_tensor_allF_bal = train_test_tensors_allF_bal[0]\n",
    "# y_train_tensor_allF_bal = train_test_tensors_allF_bal[1]\n",
    "# X_test_tensor_allF_bal = train_test_tensors_allF_bal[2]\n",
    "# y_test_tensor_allF_bal = train_test_tensors_allF_bal[3]\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# cv_scores_wide = []\n",
    "# for train, test in kfold.split(X_train_tensor_allF_bal, y_train_tensor_allF_bal):\n",
    "#     # Create model, train and get accuracy\n",
    "#     model = Wide(n_features)\n",
    "#     acc, model = model_train(model, X_train_tensor_allF_bal[train], y_train_tensor_allF_bal[train],\n",
    "#                       X_train_tensor_allF_bal[test], y_train_tensor_allF_bal[test])\n",
    "#     print(f\"Accuracy (wide): {acc:.4f}\")\n",
    "#     cv_scores_wide.append(acc)\n",
    "# cv_scores_deep = []\n",
    "# for train, test in kfold.split(X_train_tensor_allF_bal, y_train_tensor_allF_bal):\n",
    "#     # Create model, train and get accuracy\n",
    "#     model = Deep(n_features)\n",
    "#     acc, model = model_train(model, X_train_tensor_allF_bal[train], y_train_tensor_allF_bal[train],\n",
    "#                       X_train_tensor_allF_bal[test], y_train_tensor_allF_bal[test])\n",
    "#     print(f\"Accuracy (deep): {acc:.4f}\")\n",
    "#     cv_scores_deep.append(acc)\n",
    "\n",
    "# # Evaluate the model\n",
    "# wide_acc = np.mean(cv_scores_wide)\n",
    "# wide_std = np.std(cv_scores_wide)\n",
    "# deep_acc = np.mean(cv_scores_deep)\n",
    "# deep_std = np.std(cv_scores_deep)\n",
    "# print(f\"Wide: {wide_acc*100:.2f}% (+/- {wide_std*100:.2f}%)\")\n",
    "# print(f\"Deep: {deep_acc*100:.2f}% (+/- {deep_std*100:.2f}%)\")\n",
    "\n",
    "# # Rebuild model with full training set\n",
    "# if wide_acc > deep_acc:\n",
    "#     print(\"Retraining Wide model\")\n",
    "#     model = Wide(n_features)\n",
    "# else:\n",
    "#     print(\"Retraining Deep model\")\n",
    "#     model = Deep(n_features)\n",
    "# acc, model = model_train(model, X_train_tensor_allF_bal, y_train_tensor_allF_bal,\n",
    "#                   X_test_tensor_allF_bal, y_test_tensor_allF_bal)\n",
    "# print(f\"Accuracy (final): {acc:.4f}\")\n",
    "\n",
    "# # Save model\n",
    "# save_file(model, os.path.join(models_dir, 'deepL_allF_bal_model.pkl'))\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Test out inference with 5 samples\n",
    "#     for i in range(5):\n",
    "#         y_pred = model(X_test_tensor_allF_bal[i:i+1])\n",
    "#         print(f\"{X_test_tensor_allF_bal[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test_tensor_allF_bal[i].numpy()})\")\n",
    "        \n",
    "#     # Plot the ROC curve\n",
    "#     y_pred = model(X_test_tensor_allF_bal)\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test_tensor_allF_bal, y_pred)\n",
    "#     plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n",
    "#     plt.title(\"Receiver Operating Characteristics\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Using all features (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to tensors PyTorch tensors as this is the format a PyTorch model needs\n",
    "train_test_tensors_allF_bal = get_train_test_tensors(X_train_bal, y_train_bal, \n",
    "                                                     X_test_bal, y_test_bal)\n",
    "X_train_tensor_allF_bal = train_test_tensors_allF_bal[0]\n",
    "y_train_tensor_allF_bal = train_test_tensors_allF_bal[1]\n",
    "X_test_tensor_allF_bal = train_test_tensors_allF_bal[2]\n",
    "y_test_tensor_allF_bal = train_test_tensors_allF_bal[3]\n",
    "\n",
    "train_loader_allF_bal = getLoader(X_train_tensor_allF_bal, y_train_tensor_allF_bal)\n",
    "\n",
    "model_allF_bal = FeedForwardNN(input_dim=X_train_tensor_allF_bal.shape[1])\n",
    "optimizer_allF_bal = optim.SGD(model_allF_bal.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "print(\"Training neural network model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_allF_bal.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_allF_bal:\n",
    "        optimizer_allF_bal.zero_grad()\n",
    "        output = model_allF_bal(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_allF_bal.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader_allF_bal):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model_allF_bal.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_allF_bal(X_test_tensor_allF_bal)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred = predicted_classes.numpy()\n",
    "    y_true = y_test_tensor_allF_bal.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "save_file(model_allF_bal, os.path.join(models_dir, 'deepL_allF_bal_model.pkl'))\n",
    "# Save model performance\n",
    "classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "save_model_performance(classification_report_dict, 'deepL_allF_bal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Using manually selected features (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal_manSelF = X_train_bal[manual_important_features]\n",
    "X_test_bal_manSelF = X_test_bal[manual_important_features]\n",
    "train_test_tensors_manSelF_bal = get_train_test_tensors(X_train_bal_manSelF, y_train_bal,\n",
    "                                                     X_test_bal_manSelF, y_test_bal)\n",
    "X_train_tensor_manSelF_bal = train_test_tensors_manSelF_bal[0]\n",
    "y_train_tensor_manSelF_bal = train_test_tensors_manSelF_bal[1]\n",
    "X_test_tensor_manSelF_bal = train_test_tensors_manSelF_bal[2]\n",
    "y_test_tensor_manSelF_bal = train_test_tensors_manSelF_bal[3]\n",
    "\n",
    "train_loader_manSelF_bal = getLoader(X_train_tensor_manSelF_bal, y_train_tensor_manSelF_bal)\n",
    "\n",
    "model_manSelF_bal = FeedForwardNN(input_dim=X_train_tensor_manSelF_bal.shape[1])\n",
    "optimizer_manSelF_bal = optim.SGD(model_manSelF_bal.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "print(\"Training neural network model with important features...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_manSelF_bal.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_manSelF_bal:\n",
    "        optimizer_manSelF_bal.zero_grad()\n",
    "        output = model_manSelF_bal(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_manSelF_bal.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader_manSelF_bal):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model_manSelF_bal.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_manSelF_bal(X_test_tensor_manSelF_bal)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred = predicted_classes.numpy()\n",
    "    y_true = y_test_tensor_manSelF_bal.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "save_file(model_manSelF_bal, os.path.join(models_dir, 'deepL_manSelF_bal_model.pkl'))\n",
    "# Save model performance\n",
    "classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "save_model_performance(classification_report_dict, 'deepL_manSelF_bal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Using selected features (balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal_selF = X_train_bal[important_features]\n",
    "X_test_bal_selF = X_test_bal[important_features]\n",
    "train_test_tensors_selF_bal = get_train_test_tensors(X_train_bal_selF, y_train_bal,\n",
    "                                                     X_test_bal_selF, y_test_bal)\n",
    "X_train_tensor_selF_bal = train_test_tensors_selF_bal[0]\n",
    "y_train_tensor_selF_bal = train_test_tensors_selF_bal[1]\n",
    "X_test_tensor_selF_bal = train_test_tensors_selF_bal[2]\n",
    "y_test_tensor_selF_bal = train_test_tensors_selF_bal[3]\n",
    "\n",
    "train_loader_selF_bal = getLoader(X_train_tensor_selF_bal, y_train_tensor_selF_bal)\n",
    "\n",
    "model_selF_bal = FeedForwardNN(input_dim=X_train_tensor_selF_bal.shape[1])\n",
    "optimizer_selF_bal = optim.SGD(model_selF_bal.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "print(\"Training neural network model with selected features...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_selF_bal.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_selF_bal:\n",
    "        optimizer_selF_bal.zero_grad()\n",
    "        output = model_selF_bal(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_selF_bal.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader_selF_bal):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model_selF_bal.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_selF_bal(X_test_tensor_selF_bal)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred = predicted_classes.numpy()\n",
    "    y_true = y_test_tensor_selF_bal.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "save_file(model_selF_bal, os.path.join(models_dir, 'deepL_selF_bal_model.pkl'))\n",
    "# Save model performance\n",
    "classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "save_model_performance(classification_report_dict, 'deepL_selF_bal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Using selected features (imbalanced dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model over prioritizes the majority class (0 - no diabetes), as it was trained with the imbalanced data.**\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Balanced data (6.2) - Lower accuracy (74%) but much better at detecting class 1 - diabetes (79%), as it gives the same importance to both classes.\n",
    "\n",
    "Imbalanced data (6.3) - Higher accuracy (86%) but poor detection of class 1 - diabetes (16%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the imbalanced training and test sets to include only important features\n",
    "X_train_imb_selF = X_train_imb[important_features]\n",
    "X_test_imb_selF = X_test_imb[important_features]\n",
    "train_test_tensors_selF_imb = get_train_test_tensors(X_train_imb_selF, y_train_imb,\n",
    "                                                     X_test_imb_selF, y_test_imb)\n",
    "X_train_tensor_selF_imb = train_test_tensors_selF_imb[0]\n",
    "y_train_tensor_selF_imb = train_test_tensors_selF_imb[1]\n",
    "X_test_tensor_selF_imb = train_test_tensors_selF_imb[2]\n",
    "y_test_tensor_selF_imb = train_test_tensors_selF_imb[3]\n",
    "\n",
    "train_loader_selF_imb = getLoader(X_train_tensor_selF_imb, y_train_tensor_selF_imb)\n",
    "\n",
    "model_selF_imb = FeedForwardNN(input_dim=X_train_tensor_selF_imb.shape[1])\n",
    "optimizer_selF_imb = optim.SGD(model_selF_imb.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "print(\"Training neural network model with selected features on imbalanced data...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_selF_imb.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_selF_imb:\n",
    "        optimizer_selF_imb.zero_grad()\n",
    "        output = model_selF_imb(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_selF_imb.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader_selF_imb):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model_selF_imb.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_selF_imb(X_test_tensor_selF_imb)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred = predicted_classes.numpy()\n",
    "    y_true = y_test_tensor_selF_imb.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Check class distribution in predictions\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    pred_distribution = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {pred_distribution}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "save_file(model_selF_imb, os.path.join(models_dir, 'deepL_selF_imb_model.pkl'))\n",
    "# Save model performance\n",
    "classification_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "save_model_performance(classification_report_dict, 'deepL_selF_imb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets to include only important features\n",
    "X_train_bal_selF = X_train_bal[important_features]\n",
    "X_test_bal_selF = X_test_bal[important_features]\n",
    "X_val_bal_selF = X_val_bal[important_features]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_selF = scaler.fit_transform(X_train_bal_selF)\n",
    "X_test_scaled_selF = scaler.transform(X_test_bal_selF)\n",
    "X_val_scaled_selF = scaler.transform(X_val_bal_selF)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor_selF = torch.tensor(X_train_scaled_selF, dtype=torch.float32)\n",
    "y_train_tensor_selF = torch.tensor(y_train_bal.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor_selF = torch.tensor(X_test_scaled_selF, dtype=torch.float32)\n",
    "y_test_tensor_selF = torch.tensor(y_test_bal.values, dtype=torch.long)\n",
    "\n",
    "X_val_tensor_selF = torch.tensor(X_val_scaled_selF, dtype=torch.float32)\n",
    "y_val_tensor_selF = torch.tensor(y_val_bal.values, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset_selF = TensorDataset(X_train_tensor_selF, y_train_tensor_selF)\n",
    "train_loader_selF = DataLoader(train_dataset_selF, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model with input dimension matching the number of selected features\n",
    "model_selF = FeedForwardNN(input_dim=len(important_features))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_selF = optim.SGD(model_selF.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "print(\"Training neural network model with selected features and validation...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model_selF.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader_selF:\n",
    "        optimizer_selF.zero_grad()\n",
    "        output = model_selF(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_selF.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model_selF.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model_selF(X_val_tensor_selF)\n",
    "        val_loss = criterion(val_output, y_val_tensor_selF)\n",
    "        val_predicted = torch.argmax(val_output, dim=1)\n",
    "        val_acc = (val_predicted == y_val_tensor_selF).float().mean().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_selF):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "model_selF.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_selF(X_test_tensor_selF)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred = predicted_classes.numpy()\n",
    "    y_true = y_test_tensor_selF.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    test_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nBalanced Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nBalanced Test Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate validation set\n",
    "with torch.no_grad():\n",
    "    val_predictions = model_selF(X_val_tensor_selF)\n",
    "    val_predicted_classes = torch.argmax(val_predictions, dim=1)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    y_pred_val = val_predicted_classes.numpy()\n",
    "    y_val_true = y_val_tensor_selF.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    val_acc = accuracy_score(y_val_true, y_pred_val)\n",
    "    print(f\"\\nBalanced Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nBalanced Validation Classification Report:\")\n",
    "    print(classification_report(y_val_true, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Applying Differential Privacy Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.1 Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opacus\n",
    "\n",
    "print(np.__version__) # min 1.21\n",
    "print(opacus.__version__) # min 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6.1.1 allF_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [1.0, 3.0, 5.0, 8.0, 10.0]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    print(f\"\\nTraining with ε = {eps}\")\n",
    "\n",
    "    # Data and loader\n",
    "    train_test_tensors = get_train_test_tensors(X_train_bal, y_train_bal, X_test_bal, y_test_bal)\n",
    "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = train_test_tensors\n",
    "    train_loader = getLoader(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = FeedForwardNN(input_dim=X_train_tensor.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Make model private\n",
    "    privacy_engine = opacus.PrivacyEngine()\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        target_epsilon=eps,\n",
    "        target_delta=1e-5,\n",
    "        max_grad_norm=1.0,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        y_pred = predicted_classes.numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        final_epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "\n",
    "    # Save results\n",
    "    results[eps] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"epsilon\": final_epsilon\n",
    "    }\n",
    "\n",
    "    # Save model and performance\n",
    "    model_filename = f\"deepL_allF_bal_dp_model_eps{eps}.pkl\"\n",
    "    performance_filename = f\"deepL_allF_bal_dp_eps{eps}\"\n",
    "    save_file(model, os.path.join(models_dir, model_filename))\n",
    "    save_model_performance(report, performance_filename)\n",
    "\n",
    "# Print summary\n",
    "for eps, res in results.items():\n",
    "    print(f\"\\nε = {eps} | Final ε = {res['epsilon']:.2f} | Accuracy = {res['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6.1.2 selF_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [1.0, 3.0, 5.0, 8.0, 10.0]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    print(f\"\\nTraining with ε = {eps} (selected features)\")\n",
    "\n",
    "    # Subset features\n",
    "    X_train_selF = X_train_bal[important_features]\n",
    "    X_test_selF = X_test_bal[important_features]\n",
    "    train_test_tensors = get_train_test_tensors(X_train_selF, y_train_bal, X_test_selF, y_test_bal)\n",
    "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = train_test_tensors\n",
    "    train_loader = getLoader(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = FeedForwardNN(input_dim=X_train_tensor.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Make model private\n",
    "    privacy_engine = opacus.PrivacyEngine()\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        target_epsilon=eps,\n",
    "        target_delta=1e-5,\n",
    "        max_grad_norm=1.0,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        y_pred = predicted_classes.numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        final_epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "\n",
    "    # Save results\n",
    "    results[eps] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"epsilon\": final_epsilon\n",
    "    }\n",
    "\n",
    "    # Save model and performance\n",
    "    model_filename = f\"deepL_selF_bal_dp_model_eps{eps}.pkl\"\n",
    "    performance_filename = f\"deepL_selF_bal_dp_eps{eps}\"\n",
    "    save_file(model, os.path.join(models_dir, model_filename))\n",
    "    save_model_performance(report, performance_filename)\n",
    "\n",
    "# Print summary\n",
    "for eps, res in results.items():\n",
    "    print(f\"\\nε = {eps} | Final ε = {res['epsilon']:.2f} | Accuracy = {res['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6.1.3 manSelF_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [1.0, 3.0, 5.0, 8.0, 10.0]\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    print(f\"\\nTraining with ε = {eps} (manual selected features)\")\n",
    "\n",
    "    # Subset manually selected features\n",
    "    X_train_manSelF = X_train_bal[manual_important_features]\n",
    "    X_test_manSelF = X_test_bal[manual_important_features]\n",
    "    train_test_tensors = get_train_test_tensors(X_train_manSelF, y_train_bal, X_test_manSelF, y_test_bal)\n",
    "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = train_test_tensors\n",
    "    train_loader = getLoader(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = FeedForwardNN(input_dim=X_train_tensor.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Make model private\n",
    "    privacy_engine = opacus.PrivacyEngine()\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        target_epsilon=eps,\n",
    "        target_delta=1e-5,\n",
    "        max_grad_norm=1.0,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        y_pred = predicted_classes.numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        final_epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "\n",
    "    # Save results\n",
    "    results[eps] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"epsilon\": final_epsilon\n",
    "    }\n",
    "\n",
    "    # Save model and performance\n",
    "    model_filename = f\"deepL_manSelF_bal_dp_model_eps{eps}.pkl\"\n",
    "    performance_filename = f\"deepL_manSelF_bal_dp_eps{eps}\"\n",
    "    save_file(model, os.path.join(models_dir, model_filename))\n",
    "    save_model_performance(report, performance_filename)\n",
    "\n",
    "# Print summary\n",
    "for eps, res in results.items():\n",
    "    print(f\"\\nε = {eps} | Final ε = {res['epsilon']:.2f} | Accuracy = {res['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define feature sets and epsilon values\n",
    "feature_sets = ['allF_bal', 'selF_bal', 'manSelF_bal']\n",
    "epsilon_values = [1.0, 3.0, 5.0, 8.0, 10.0]\n",
    "\n",
    "# Build list of model file paths\n",
    "model_files = []\n",
    "\n",
    "for fs in feature_sets:\n",
    "    # Non-private model\n",
    "    model_files.append(f'models/deepL_{fs}_model.pkl')\n",
    "    \n",
    "    # DP models with specific epsilon values\n",
    "    for eps in epsilon_values:\n",
    "        model_files.append(f'models/deepL_{fs}_dp_model_eps{eps}.pkl')\n",
    "\n",
    "# Load all models\n",
    "loaded_models = {}\n",
    "for model_path in model_files:\n",
    "    if os.path.exists(model_path):\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model_name = os.path.basename(model_path).replace('.pkl', '')\n",
    "            loaded_models[model_name] = pickle.load(file)\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Model Inversion Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# Define feature sets and epsilon values\n",
    "feature_sets = ['allF_bal', 'selF_bal', 'manSelF_bal']\n",
    "epsilon_values = [1.0, 3.0, 5.0, 8.0, 10.0]\n",
    "\n",
    "# Build model file paths\n",
    "model_files = []\n",
    "\n",
    "for fs in feature_sets:\n",
    "    # Non-private model\n",
    "    model_files.append(f'models/deepL_{fs}_model.pkl')\n",
    "    \n",
    "    # DP models with specific epsilons\n",
    "    for eps in epsilon_values:\n",
    "        model_files.append(f'models/deepL_{fs}_dp_model_eps{eps}.pkl')\n",
    "\n",
    "# Load models and make deep copies\n",
    "loaded_models = {}\n",
    "model_copies = {}\n",
    "\n",
    "for model_path in model_files:\n",
    "    if os.path.exists(model_path):\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model_name = os.path.basename(model_path).replace('.pkl', '')\n",
    "            model = pickle.load(file)\n",
    "            loaded_models[model_name] = model\n",
    "            model_copies[model_name + '_copy'] = copy.deepcopy(model)\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inversion_attack_nn(model, target_class, feature_names, X_train, scaler=None, iterations=300):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get min/max values for each feature\n",
    "    min_values = X_train[feature_names].min()\n",
    "    max_values = X_train[feature_names].max()\n",
    "    \n",
    "    # Start with random values for the features\n",
    "    sample = {}\n",
    "    for feature in feature_names:\n",
    "        sample[feature] = np.random.uniform(min_values[feature], max_values[feature])\n",
    "    \n",
    "    # Save best sample and confidence\n",
    "    best_sample = sample.copy()\n",
    "    best_confidence = 0\n",
    "    \n",
    "    # Optimize sample through iterations\n",
    "    for i in range(iterations):\n",
    "        # Create DataFrame and convert to tensor\n",
    "        current_df = pd.DataFrame([sample])\n",
    "\n",
    "        if scaler:\n",
    "            current_data = scaler.transform(current_df)\n",
    "        else:\n",
    "            current_data = current_df.values\n",
    "\n",
    "        current_tensor = torch.tensor(current_data, dtype=torch.float32)\n",
    "\n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(current_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            confidence = probabilities[0][target_class].item()\n",
    "\n",
    "        # Save if better than previous\n",
    "        if confidence > best_confidence:\n",
    "            best_confidence = confidence\n",
    "            best_sample = {f: round(v) for f, v in sample.items()}\n",
    "\n",
    "        # Optimize one feature at a time with ±1 steps\n",
    "        for feature in feature_names:\n",
    "            original = sample[feature]\n",
    "\n",
    "            # Try +1\n",
    "            sample[feature] = min(max_values[feature], original + 1)\n",
    "            plus_df = pd.DataFrame([sample])\n",
    "            plus_data = scaler.transform(plus_df) if scaler else plus_df.values\n",
    "            plus_tensor = torch.tensor(plus_data, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                plus_output = model(plus_tensor)\n",
    "                plus_conf = torch.nn.functional.softmax(plus_output, dim=1)[0][target_class].item()\n",
    "\n",
    "            # Try -1\n",
    "            sample[feature] = max(min_values[feature], original - 1)\n",
    "            minus_df = pd.DataFrame([sample])\n",
    "            minus_data = scaler.transform(minus_df) if scaler else minus_df.values\n",
    "            minus_tensor = torch.tensor(minus_data, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                minus_output = model(minus_tensor)\n",
    "                minus_conf = torch.nn.functional.softmax(minus_output, dim=1)[0][target_class].item()\n",
    "\n",
    "            # Choose the best direction\n",
    "            if plus_conf > minus_conf and plus_conf > confidence:\n",
    "                sample[feature] = min(max_values[feature], original + 1)\n",
    "            elif minus_conf > plus_conf and minus_conf > confidence:\n",
    "                sample[feature] = max(min_values[feature], original - 1)\n",
    "            else:\n",
    "                sample[feature] = original\n",
    "\n",
    "    # Find most similar training example\n",
    "    reconstructed_df = pd.DataFrame([best_sample])\n",
    "    \n",
    "    # Calculate distance to each training example\n",
    "    distances = []\n",
    "    for _, row in X_train.iterrows():\n",
    "        dist = 0\n",
    "        for feature in feature_names:\n",
    "            dist += (row[feature] - best_sample[feature])**2\n",
    "        distances.append(np.sqrt(dist))\n",
    "    \n",
    "    most_similar_idx = np.argmin(distances)\n",
    "    most_similar = X_train.iloc[[most_similar_idx]]\n",
    "    \n",
    "    return {\n",
    "        'reconstructed': reconstructed_df,\n",
    "        'most_similar': most_similar,\n",
    "        'confidence': best_confidence,\n",
    "        'similarity': 1.0 / (1.0 + np.min(distances))  # Convert distance to similarity (0-1)\n",
    "    }\n",
    "\n",
    "def analyze_model_inversion_attack(attack_results, feature_names):\n",
    "    print(f\"Attack completed with confidence: {attack_results['confidence']:.4f}\")\n",
    "    print(f\"Similarity to closest training sample: {attack_results['similarity']:.4f}\")\n",
    "    \n",
    "    # Visualize comparison between reconstructed and original sample\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get data for visualization\n",
    "    recon = attack_results['reconstructed']\n",
    "    orig = attack_results['most_similar']\n",
    "    \n",
    "    # Plot all features\n",
    "    display_features = feature_names\n",
    "    \n",
    "    x = np.arange(len(display_features))\n",
    "    width = 0.3\n",
    "    \n",
    "    plt.bar(x - width/2, [recon[f].values[0] for f in display_features], width, label='Reconstructed')\n",
    "    plt.bar(x + width/2, [orig[f].values[0] for f in display_features], width, label='Original')\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Model Inversion Attack: Reconstructed vs. Original Sample')\n",
    "    plt.xticks(x, display_features, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare all features in detail\n",
    "    print(\"\\n FEATURE-SPECIFIC ANALYSIS\")\n",
    "    for feature in feature_names:\n",
    "        recon_value = recon[feature].values[0]\n",
    "        orig_value = orig[feature].values[0]\n",
    "        \n",
    "        diff = abs(recon_value - orig_value)\n",
    "        abs_diff = abs(recon_value - orig_value)\n",
    "\n",
    "        if abs(orig_value) < 1e-5:\n",
    "            percent_diff = 0.0\n",
    "        else:\n",
    "            percent_diff = (abs_diff / abs(orig_value)) * 100\n",
    "\n",
    "        print(f\"Feature: {feature}\")\n",
    "        print(f\"  Original value: {orig_value:.2f}\")\n",
    "        print(f\"  Reconstructed value: {recon_value:.2f}\")\n",
    "        print(f\"  Absolute difference: {diff:.2f}\")\n",
    "        print(f\"  Percentage difference: {percent_diff:.2f}%\")\n",
    "    \n",
    "    return attack_results\n",
    "\n",
    "def run_attack_on_model(model, target_class, feature_names, X_train, scaler=None):\n",
    "    # Run model inversion attack on a specific model\n",
    "    results = model_inversion_attack_nn(\n",
    "        model=model,\n",
    "        target_class=target_class,\n",
    "        feature_names=feature_names,\n",
    "        X_train=X_train,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    \n",
    "    analyze_model_inversion_attack(results, feature_names)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_scaler_for_model(X_data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_data)\n",
    "    return scaler\n",
    "\n",
    "# Define feature sets and corresponding training data\n",
    "feature_sets = {\n",
    "    \"allF_bal\": X_train_bal,\n",
    "    \"manSelF_bal\": X_train_bal[manual_important_features],\n",
    "    \"selF_bal\": X_train_bal[important_features]\n",
    "}\n",
    "\n",
    "# Create scalers for each feature set\n",
    "scalers = {name: create_scaler_for_model(X) for name, X in feature_sets.items()}\n",
    "\n",
    "# Run model inversion attack and collect results\n",
    "attack_results = {}\n",
    "for fs_name in feature_sets:\n",
    "    model_key = f\"deepL_{fs_name}_model_copy\"\n",
    "    model = model_copies.get(model_key)\n",
    "    if model:\n",
    "        print(f\"\\nMODEL INVERSION ATTACK: {fs_name.upper()}\")\n",
    "        results = model_inversion_attack_nn(\n",
    "            model=model,\n",
    "            target_class=1,\n",
    "            feature_names=feature_sets[fs_name].columns.tolist(),\n",
    "            X_train=feature_sets[fs_name],\n",
    "            scaler=scalers[fs_name]\n",
    "        )\n",
    "        analyze_model_inversion_attack(results, feature_sets[fs_name].columns.tolist())\n",
    "        attack_results[fs_name] = results\n",
    "    else:\n",
    "        print(f\"Model not found: {model_key}\")\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\nATTACK EFFECTIVENESS COMPARISON\")\n",
    "print(f\"{'Model':<25} {'Confidence':<15} {'Similarity':<15}\")\n",
    "print(f\"{'-'*25:<25} {'-'*15:<15} {'-'*15:<15}\")\n",
    "for fs_name, result in attack_results.items():\n",
    "    label = fs_name.replace('_bal', '').replace('manSelF', 'Manual Selected Features').replace('selF', 'Selected Features').replace('allF', 'All Features')\n",
    "    print(f\"{label:<25} {result['confidence']:<15.4f} {result['similarity']:<15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Membership Inference Attack(MIA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---1. Standard MIA---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this attack is to determine whether a given sample was used in training. \n",
    "\n",
    "The more overfitted a machine learning model is, the easier it will be for an adversary to stage membership inference attacks against it. Therefore, a machine model that generalizes well on unseen examples is also more secure against membership inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_membership_inference_attack(model, X_train, X_test, scaler=None, n_samples=500):\n",
    "    model.eval()\n",
    "    X_train_sample = X_train.sample(n=min(n_samples, len(X_train)))\n",
    "    X_test_sample = X_test.sample(n=min(n_samples, len(X_test)))\n",
    "\n",
    "    member_confidences = []\n",
    "    for _, sample in X_train_sample.iterrows():\n",
    "        df_sample = pd.DataFrame([sample])\n",
    "        sample_data = scaler.transform(df_sample) if scaler else df_sample.values\n",
    "        sample_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            output = model(sample_tensor)\n",
    "            confidence = torch.nn.functional.softmax(output, dim=1)[0].max().item()\n",
    "        member_confidences.append(confidence)\n",
    "\n",
    "    non_member_confidences = []\n",
    "    for _, sample in X_test_sample.iterrows():\n",
    "        df_sample = pd.DataFrame([sample])\n",
    "        sample_data = scaler.transform(df_sample) if scaler else df_sample.values\n",
    "        sample_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            output = model(sample_tensor)\n",
    "            confidence = torch.nn.functional.softmax(output, dim=1)[0].max().item()\n",
    "        non_member_confidences.append(confidence)\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    tpr_list, fpr_list = [], []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        tp = sum(conf > threshold for conf in member_confidences)\n",
    "        fn = sum(conf <= threshold for conf in member_confidences)\n",
    "        tn = sum(conf <= threshold for conf in non_member_confidences)\n",
    "        fp = sum(conf > threshold for conf in non_member_confidences)\n",
    "        tpr = tp / (tp + fn) if (tp + fn) else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) else 0\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    roc_points = sorted(zip(fpr_list, tpr_list))\n",
    "    sorted_fpr, sorted_tpr = zip(*roc_points) if roc_points else ([], [])\n",
    "    roc_auc = np.trapz(y=sorted_tpr, x=sorted_fpr) if sorted_fpr else 0.5\n",
    "\n",
    "    distances = np.sqrt((1 - np.array(tpr_list))**2 + np.array(fpr_list)**2)\n",
    "    optimal_idx = np.argmin(distances)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    accuracy = (sum(conf > optimal_threshold for conf in member_confidences) +\n",
    "                sum(conf <= optimal_threshold for conf in non_member_confidences)) / \\\n",
    "               (len(member_confidences) + len(non_member_confidences))\n",
    "\n",
    "    return {\n",
    "        'member_confidences': member_confidences,\n",
    "        'non_member_confidences': non_member_confidences,\n",
    "        'thresholds': thresholds,\n",
    "        'tpr': tpr_list,\n",
    "        'fpr': fpr_list,\n",
    "        'sorted_tpr': sorted_tpr,\n",
    "        'sorted_fpr': sorted_fpr,\n",
    "        'roc_auc': roc_auc,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'optimal_tpr': tpr_list[optimal_idx],\n",
    "        'optimal_fpr': fpr_list[optimal_idx],\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def plot_membership_inference_results(results, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    sns.histplot(results['member_confidences'], color='blue', alpha=0.5, label='Members (training)', bins=30, kde=True, ax=ax1)\n",
    "    sns.histplot(results['non_member_confidences'], color='red', alpha=0.5, label='Non-members (test)', bins=30, kde=True, ax=ax1)\n",
    "    ax1.axvline(x=results['optimal_threshold'], color='green', linestyle='--', label=f'Threshold: {results[\"optimal_threshold\"]:.3f}')\n",
    "    ax1.set_title(f'Confidence Distribution - {model_name}')\n",
    "    ax1.set_xlabel('Confidence Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(results['sorted_fpr'], results['sorted_tpr'], color='blue', lw=2, label=f'ROC AUC = {results[\"roc_auc\"]:.3f}')\n",
    "    ax2.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    ax2.scatter([results['optimal_fpr']], [results['optimal_tpr']], color='red', s=100, label='Optimal Point')\n",
    "    ax2.set_title(f'ROC Curve - {model_name}')\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.set_ylim([0, 1.05])\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_and_compare_models(models_dict, X_train, X_test, feature_sets_dict, scalers_dict):\n",
    "    results = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        features = feature_sets_dict[model_name]\n",
    "        scaler = scalers_dict[model_name]\n",
    "        print(f\"\\nEvaluating model: {model_name}\")\n",
    "        result = evaluate_membership_inference_attack(\n",
    "            model, X_train[features], X_test[features], scaler\n",
    "        )\n",
    "        results[model_name] = result\n",
    "        plot_membership_inference_results(result, model_name)\n",
    "    return results\n",
    "\n",
    "def plot_comparison_metrics(results_dict):\n",
    "    # Print available models for debugging\n",
    "    print(\"Available models in results:\", list(results_dict.keys()))\n",
    "    \n",
    "    # Define specific model groups for comparison using the actual model names from results_dict\n",
    "    all_features_models = {\n",
    "        'baseline': 'deepL_allF_bal_model',\n",
    "        'eps1.0': 'deepL_allF_bal_dp_model_eps1.0',\n",
    "        'eps10.0': 'deepL_allF_bal_dp_model_eps10.0'\n",
    "    }\n",
    "    \n",
    "    selected_features_models = {\n",
    "        'baseline': 'deepL_selF_bal_model',\n",
    "        'eps1.0': 'deepL_selF_bal_dp_model_eps1.0',\n",
    "        'eps10.0': 'deepL_selF_bal_dp_model_eps10.0'\n",
    "    }\n",
    "    \n",
    "    # Print detailed comparison table\n",
    "    print(\"\\nMEMBERSHIP INFERENCE ATTACK RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<40} {'ROC AUC':<12} {'Accuracy':<12} {'Threshold':<12}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nALL FEATURES MODELS:\")\n",
    "    print(\"-\"*40)\n",
    "    for label, model_name in all_features_models.items():\n",
    "        if model_name in results_dict:\n",
    "            result = results_dict[model_name]\n",
    "            print(f\"{label:<40} {result['roc_auc']:<12.4f} {result['accuracy']:<12.4f} {result['optimal_threshold']:<12.4f}\")\n",
    "    \n",
    "    print(\"\\nSELECTED FEATURES MODELS:\")\n",
    "    print(\"-\"*40)\n",
    "    for label, model_name in selected_features_models.items():\n",
    "        if model_name in results_dict:\n",
    "            result = results_dict[model_name]\n",
    "            print(f\"{label:<40} {result['roc_auc']:<12.4f} {result['accuracy']:<12.4f} {result['optimal_threshold']:<12.4f}\")\n",
    "\n",
    "# Models that we want to evaluate\n",
    "target_models = [\n",
    "    'deepL_allF_bal_model',\n",
    "    'deepL_allF_bal_dp_model_eps1.0', \n",
    "    'deepL_allF_bal_dp_model_eps10.0',\n",
    "    'deepL_selF_bal_model',\n",
    "    'deepL_selF_bal_dp_model_eps1.0',\n",
    "    'deepL_selF_bal_dp_model_eps10.0'\n",
    "]\n",
    "\n",
    "# Build model dictionary - filter from model_copies to only include target models\n",
    "models = {}\n",
    "for model_name in target_models:\n",
    "    model_key = model_name + '_copy'  # Add '_copy' suffix as in original code\n",
    "    if model_key in model_copies:\n",
    "        models[model_name] = model_copies[model_key]\n",
    "    else:\n",
    "        print(f\"Warning: {model_key} not found in model_copies\")\n",
    "\n",
    "# Build feature sets and scalers for each model\n",
    "feature_sets = {}\n",
    "scalers = {}\n",
    "\n",
    "for model_name in models:\n",
    "    if 'allF' in model_name:\n",
    "        # All features models\n",
    "        features = X_train_bal.columns.tolist()\n",
    "        data = X_train_bal\n",
    "    elif 'selF' in model_name:\n",
    "        # Selected features models\n",
    "        features = important_features\n",
    "        data = X_train_bal[important_features]\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    feature_sets[model_name] = features\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    scalers[model_name] = scaler\n",
    "\n",
    "# Run evaluation and plot\n",
    "print(f\"\\nRUNNING MEMBERSHIP INFERENCE ATTACK EVALUATION ON {len(models)} TARGET MODELS\") # Just to verify that all models are loaded\n",
    "print(\"Target models:\", list(models.keys()))\n",
    "\n",
    "if models:\n",
    "    results = evaluate_and_compare_models(models, X_train_bal, X_test_bal, feature_sets, scalers)\n",
    "    plot_comparison_metrics(results)\n",
    "else:\n",
    "    print(\"No models found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 2. Real World Case MIA ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership_inference_attack_practical(models, feature_sets, scalers, X_train_bal, X_test_bal, num_subjects=10):\n",
    "    \n",
    "    # Calculate attack thresholds\n",
    "    thresholds = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        features = feature_sets[model_name]\n",
    "        scaler = scalers[model_name]\n",
    "        \n",
    "        # Ensure features exist\n",
    "        available_features = [f for f in features if f in X_train_bal.columns]\n",
    "        if len(available_features) != len(features):\n",
    "            continue\n",
    "        \n",
    "        # Get member/non-member samples\n",
    "        train_sample = X_train_bal[available_features].sample(500) if len(X_train_bal) > 500 else X_train_bal[available_features]\n",
    "        test_sample = X_test_bal[available_features].sample(500) if len(X_test_bal) > 500 else X_test_bal[available_features]\n",
    "        \n",
    "        # Calculate confidence scores\n",
    "        train_confidences = []\n",
    "        test_confidences = []\n",
    "        \n",
    "        for i in range(len(train_sample)):\n",
    "            try:\n",
    "                sample = train_sample.iloc[i]\n",
    "                df_sample = pd.DataFrame([sample])\n",
    "                sample_data = scaler.transform(df_sample) if scaler else df_sample.values\n",
    "                sample_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(sample_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                    confidence = probabilities[0].max().item()\n",
    "                \n",
    "                train_confidences.append(confidence)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        for i in range(len(test_sample)):\n",
    "            try:\n",
    "                sample = test_sample.iloc[i]\n",
    "                df_sample = pd.DataFrame([sample])\n",
    "                sample_data = scaler.transform(df_sample) if scaler else df_sample.values\n",
    "                sample_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(sample_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                    confidence = probabilities[0].max().item()\n",
    "                \n",
    "                test_confidences.append(confidence)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not train_confidences or not test_confidences:\n",
    "            continue\n",
    "            \n",
    "        # Find optimal threshold\n",
    "        thresholds_list = np.linspace(0, 1, 100)\n",
    "        best_threshold = 0.5\n",
    "        best_distance = float('inf')\n",
    "        \n",
    "        for threshold in thresholds_list:\n",
    "            tpr = sum(1 for conf in train_confidences if conf > threshold) / len(train_confidences)\n",
    "            fpr = sum(1 for conf in test_confidences if conf > threshold) / len(test_confidences)\n",
    "            distance = np.sqrt((1 - tpr)**2 + fpr**2)\n",
    "            \n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        thresholds[model_name] = best_threshold\n",
    "    \n",
    "    if not thresholds:\n",
    "        print(\"No models could be evaluated.\")\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # Get subjects for analysis\n",
    "    known_members = X_train_bal.sample(num_subjects // 2) if len(X_train_bal) >= num_subjects // 2 else X_train_bal\n",
    "    known_non_members = X_test_bal.sample(num_subjects // 2) if len(X_test_bal) >= num_subjects // 2 else X_test_bal\n",
    "    \n",
    "    all_subjects = pd.concat([known_members, known_non_members], ignore_index=True)\n",
    "    subject_true_status = [True] * len(known_members) + [False] * len(known_non_members)\n",
    "    \n",
    "    # Run attack\n",
    "    \n",
    "    # Store results\n",
    "    subject_predictions = {model_name: [] for model_name in thresholds.keys()}\n",
    "    \n",
    "    # Process each subject\n",
    "    for i, (_, subject_data) in enumerate(all_subjects.iterrows()):\n",
    "        for model_name, model in models.items():\n",
    "            if model_name not in thresholds:\n",
    "                continue\n",
    "                \n",
    "            features = feature_sets[model_name]\n",
    "            scaler = scalers[model_name]\n",
    "            threshold = thresholds[model_name]\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                filtered_data = {feature: subject_data[feature] for feature in features if feature in subject_data}\n",
    "                if len(filtered_data) != len(features):\n",
    "                    continue\n",
    "                    \n",
    "                df_sample = pd.DataFrame([filtered_data])\n",
    "                sample_data = scaler.transform(df_sample) if scaler else df_sample.values\n",
    "                sample_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "                \n",
    "                # Get prediction\n",
    "                with torch.no_grad():\n",
    "                    output = model(sample_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                    confidence = probabilities[0].max().item()\n",
    "                \n",
    "                # Determine membership\n",
    "                is_member = confidence > threshold\n",
    "                \n",
    "                # Add to predictions\n",
    "                subject_predictions[model_name].append((is_member, subject_true_status[i]))\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    success_rates = {}\n",
    "    true_positive_rates = {}\n",
    "    true_negative_rates = {}\n",
    "    \n",
    "    for model_name, predictions in subject_predictions.items():\n",
    "        if not predictions:\n",
    "            continue\n",
    "            \n",
    "        # Overall success rate\n",
    "        correct = sum(1 for pred, actual in predictions if pred == actual)\n",
    "        total = len(predictions)\n",
    "        success_rate = correct / total if total > 0 else 0\n",
    "        \n",
    "        # True positive rate\n",
    "        true_positives = sum(1 for pred, actual in predictions if pred and actual)\n",
    "        actual_positives = sum(1 for _, actual in predictions if actual)\n",
    "        tpr = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "        \n",
    "        # True negative rate\n",
    "        true_negatives = sum(1 for pred, actual in predictions if not pred and not actual)\n",
    "        actual_negatives = sum(1 for _, actual in predictions if not actual)\n",
    "        tnr = true_negatives / actual_negatives if actual_negatives > 0 else 0\n",
    "        \n",
    "        success_rates[model_name] = success_rate\n",
    "        true_positive_rates[model_name] = tpr\n",
    "        true_negative_rates[model_name] = tnr\n",
    "    \n",
    "    # Separate DP and non-DP models\n",
    "    dp_models = [name for name in success_rates.keys() if '_dp' in name]\n",
    "    non_dp_models = [name for name in success_rates.keys() if '_dp' not in name]\n",
    "    \n",
    "    dp_rates = [success_rates[name] for name in dp_models]\n",
    "    non_dp_rates = [success_rates[name] for name in non_dp_models]\n",
    "    \n",
    "    avg_dp_success = sum(dp_rates) / len(dp_rates) if dp_rates else 0\n",
    "    avg_non_dp_success = sum(non_dp_rates) / len(non_dp_rates) if non_dp_rates else 0\n",
    "    \n",
    "    # Output results table\n",
    "    print(\"\\nAttack Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<12} {'Type':<8} {'Success %':<10} {'TPR %':<10} {'TNR %':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name in sorted(success_rates.keys()):\n",
    "        model_type = \"With DP\" if '_dp' in name else \"No DP\"\n",
    "        success = success_rates[name] * 100\n",
    "        tpr = true_positive_rates[name] * 100\n",
    "        tnr = true_negative_rates[name] * 100\n",
    "        \n",
    "        print(f\"{name:<12} {model_type:<8} {success:>8.1f}% {tpr:>8.1f}% {tnr:>8.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Avg No DP':<12} {'':8} {avg_non_dp_success*100:>8.1f}%\")\n",
    "    print(f\"{'Avg With DP':<12} {'':8} {avg_dp_success*100:>8.1f}%\")\n",
    "    print(f\"{'Improvement':<12} {'':8} {(avg_non_dp_success-avg_dp_success)*100:>8.1f}%\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    x_pos = np.arange(len(success_rates))\n",
    "    model_names = list(success_rates.keys())\n",
    "    rates = [success_rates[name] * 100 for name in model_names]\n",
    "    colors = ['#3498db' if '_dp' not in name else '#e74c3c' for name in model_names]\n",
    "    \n",
    "    # Create chart\n",
    "    bars = plt.bar(x_pos, rates, color=colors)\n",
    "    \n",
    "    # Add labels\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Attack Success Rate (%)')\n",
    "    plt.title('Membership Inference Attack Success Rate by Model')\n",
    "    plt.xticks(x_pos, model_names, rotation=45)\n",
    "    \n",
    "    # Add legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    blue_patch = mpatches.Patch(color='#3498db', label='No DP')\n",
    "    red_patch = mpatches.Patch(color='#e74c3c', label='With DP')\n",
    "    plt.legend(handles=[blue_patch, red_patch])\n",
    "    \n",
    "    # Add bar labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Add reference lines\n",
    "    plt.axhline(y=50, color='gray', linestyle='--')\n",
    "    plt.axhline(y=avg_non_dp_success * 100, color='#3498db', linestyle='-', alpha=0.3)\n",
    "    plt.axhline(y=avg_dp_success * 100, color='#e74c3c', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    privacy_improvement = avg_non_dp_success - avg_dp_success\n",
    "    \n",
    "    if privacy_improvement > 0.05:\n",
    "        print(f\"Differential privacy improves privacy protection by {privacy_improvement*100:.1f}%\")\n",
    "    elif privacy_improvement > 0:\n",
    "        print(f\"Differential privacy provides modest protection ({privacy_improvement*100:.1f}% improvement)\")\n",
    "    else:\n",
    "        print(\"Current differential privacy implementation doesn't show privacy benefits\")\n",
    "    \n",
    "    return success_rates, true_positive_rates, true_negative_rates\n",
    "\n",
    "# Call the function\n",
    "attack_success, attack_tpr, attack_tnr = membership_inference_attack_practical(\n",
    "    models,\n",
    "    feature_sets,\n",
    "    scalers,\n",
    "    X_train_bal,\n",
    "    X_test_bal,\n",
    "    num_subjects=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### 6.3 Attribute Inference Attack -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get models\n",
    "# deepL_model_allF_copy = copy.deepcopy(deepL_model_allF)\n",
    "# deepL_model_manSelF_copy = copy.deepcopy(deepL_model_manSelF)\n",
    "# deepL_model_selF_copy = copy.deepcopy(deepL_model_selF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate\n",
    "\n",
    "# assert 'rf_model' in locals(), \"Deep Learning model (rf_model) is not defined!\" #if not defined, run 6.6\n",
    "\n",
    "# sensitive_feature = 'Income'\n",
    "\n",
    "# # Exclude 'Income' from training features - because we will be trying to infer this attribute\n",
    "# attack_features = [f for f in X_train.columns if f != sensitive_feature]\n",
    "\n",
    "# # Get confidence scores from the target model\n",
    "# # For each prediction, predict_proba() gives an array of probabilities for each class (8 in case of Income)\n",
    "# train_confidences = rf_model.predict_proba(X_train).max(axis=1)\n",
    "# test_confidences = rf_model.predict_proba(X_test).max(axis=1)\n",
    "\n",
    "# # Add confidence scores to the dataset\n",
    "# X_train_attack = X_train[attack_features].copy()\n",
    "# X_train_attack['model_confidence'] = train_confidences\n",
    "\n",
    "# X_test_attack = X_test[attack_features].copy()\n",
    "# X_test_attack['model_confidence'] = test_confidences\n",
    "\n",
    "\n",
    "# y_train_attack = X_train[sensitive_feature]\n",
    "# y_test_attack = X_test[sensitive_feature]\n",
    "\n",
    "# # Train the Inference Model (using deep learning)\n",
    "# rf_model.fit(X_train_attack, y_train_attack)\n",
    "\n",
    "# # Evaluate the Inference Model\n",
    "# y_pred_attack = rf_model.predict(X_test_attack)\n",
    "\n",
    "# attack_accuracy = accuracy_score(y_test_attack, y_pred_attack)\n",
    "\n",
    "# train_mean_conf = np.mean(train_confidences)\n",
    "# test_mean_conf = np.mean(test_confidences)\n",
    "\n",
    "# print(\"\\nAttribute Inference Attack on 'Income':\")\n",
    "# print(f\"Attack Accuracy: {attack_accuracy:.2f}\")\n",
    "# print(f\"Avg Train Confidence: {train_mean_conf:.2f}\")\n",
    "# print(f\"Avg Test Confidence: {test_mean_conf:.2f}\\n\")\n",
    "\n",
    "# report = classification_report(y_test_attack, y_pred_attack, output_dict=True)\n",
    "\n",
    "# def print_classification_report(report):\n",
    "#     headers = [\"Metric\"] + [str(label) for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "#     rows = [\n",
    "#         [\"Precision\"] + [report[label]['precision'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "#         [\"Recall\"] + [report[label]['recall'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "#         [\"F1-Score\"] + [report[label]['f1-score'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')],\n",
    "#         [\"Support\"] + [report[label]['support'] for label in report if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "#     ]\n",
    "\n",
    "#     # Add macro and weighted averages\n",
    "#     rows.append([\"Macro Avg\", report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score'], report['macro avg']['support']])\n",
    "#     rows.append([\"Weighted Avg\", report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score'], report['weighted avg']['support']])\n",
    "#     rows.append([\"Accuracy\", \"\", \"\", report['accuracy'], \"\"])\n",
    "\n",
    "#     print(tabulate(rows, headers=headers, tablefmt=\"grid\", floatfmt=\".2f\"))\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.hist(train_confidences, bins=50, alpha=0.6, label=\"Train (Member)\", color='blue')\n",
    "# plt.hist(test_confidences, bins=50, alpha=0.6, label=\"Test (Non-Member)\", color='red')\n",
    "# plt.axvline(train_mean_conf, color='blue', linestyle='dashed', linewidth=1, label=f\"Mean Train Confidence: {train_mean_conf:.2f}\")\n",
    "# plt.axvline(test_mean_conf, color='red', linestyle='dashed', linewidth=1, label=f\"Mean Test Confidence: {test_mean_conf:.2f}\")\n",
    "# plt.xlabel(\"Model Confidence Score\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Attribute Inference Attack - Confidence Distribution\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attack achieved 47% accuracy, meaning it correctly inferred the income level of individuals nearly half the time. As the Income attribute has 8 levels, it is normal that the accuracy is lower. If the attack tried to guess randomly, it would be around 12,5% (1/8 *100). Therefore data is leaking. \n",
    "\n",
    "Also, the model showed higher confidence on training data (0.91) than test data (0.83), indicating a potential privacy risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### 6.4 Find most exposed individuals and the analyzing the infered data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add predictions and confidence to the test dataset for analysis\n",
    "# vulnerable_data = X_test_attack.copy()\n",
    "# vulnerable_data['True_Income'] = y_test_attack.values\n",
    "# vulnerable_data['Predicted_Income'] = y_pred_attack\n",
    "\n",
    "# # Calculate prediction correctness\n",
    "# vulnerable_data['Correct_Prediction'] = (vulnerable_data['True_Income'] == vulnerable_data['Predicted_Income'])\n",
    "\n",
    "# # Fidn top 20 most vulnerable individuals (highest confidence)\n",
    "# most_vulnerable = vulnerable_data.sort_values(by='model_confidence', ascending=False).head(20)\n",
    "\n",
    "# print(\"Most Vulnerable Individuals (Top 20 by Confidence Score):\")\n",
    "# print(tabulate(most_vulnerable[['model_confidence', 'True_Income', 'Predicted_Income', 'Correct_Prediction']],\n",
    "#                headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 most vulnerable individuals had a model confidence of 1.0, meaning the model was completely certain about their predicted income. 16 out of 20 predictions were completely correct (80%), highlighting a significant privacy concern for these individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 . Evaluation & Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 . Results & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 . Conclusion & Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
