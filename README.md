# Balancing Privacy and Accuracy in Machine Learning Models with Differential Privacy

## Topics
- **Differential Privacy**
- **Machine Learning**
- **Risk Assessment**

## Overview
The use of personal data to train machine learning models enhances their functionality and precision. However, ensuring data privacy is a crucial challenge. **Differential Privacy** (DP) is a **Privacy-Enhancing Technology (PET)** designed to maintain the confidentiality of personal data while enabling model training.

This project explores how **Differential Privacy Stochastic Gradient Descent (DP-SGD)** and **Model Agnostic Private Learning (MAPL)** impact machine learning models. Specifically, we examine whether DP techniques interfere with model accuracy and the evolution of machine learning.

## Goals
- **Risk Assessment:** Evaluate potential attacks arising from the misuse of personal data and analyze how these risks can be mitigated using Differential Privacy.
- **Model Implementation:** Implement machine learning models using both raw personal data and data protected through a Differential Privacy method. Compare their accuracy and privacy levels.
- **Attack Evaluation:** Assess the trade-off between model accuracy and the risk of personal data extraction through adversarial attacks.
- **Recommendations:** Propose best practices for maintaining personal data confidentiality while fostering innovation in machine learning.

## Authors & Contact
- **Mai Hansen**, Research Assistant  
  ðŸ“§ cmch@es.aau.dk  
- **Co-Supervisor:** Qiongxiu Li (Jane)  
  ðŸ“§ qili@es.aau.dk  

## Recommended Reading
For further insights, we recommend the following resource from NIST:
ðŸ”— [How to Deploy Machine Learning with Differential Privacy](https://www.nist.gov/blogs/cybersecurity-insights/howdeploy-machine-learning-differential-privacy)
